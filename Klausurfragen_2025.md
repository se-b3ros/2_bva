# BVA Klausurfragen 2025

## 0.Overview [5]

**0.1 Wie kann man ein √ºberbestimmtes Gleichungssystem l√∂sen. Erl√§utern Sie die erforderlichen Schritte. Formen Sie die Gleichungen f√ºr ein konkretes BSP um, sodass via Matrix + Falk-Schema l√∂sbar. Welche Vor- und Nachteile besitzt diese Strategie des Gleichungsl√∂sens?**


Ein √ºberbestimmtes Gleichungssystem liegt vor, wenn die Anzahl der nicht-redundanten Gleichungen gr√∂√üer ist als die Anzahl der zu l√∂senden oder zu approximierenden Variablen. Im Allgemeinen lassen diese Gleichungssysteme keine eindeutige und exakte L√∂sung zu, weshalb ein Fehler pro Gleichung eingef√ºhrt wird.

L√∂sung eines √ºberbestimmten Gleichungssystems:

Das Ziel bei der L√∂sung eines √ºberbestimmten Gleichungssystems ist es, den Gesamtfehler als Funktion der quadrierten Differenzen zu minimieren, um die bestm√∂gliche Ann√§herung an eine L√∂sung zu finden. Dies wird durch die Umformung in die sogenannte **Gau√üsche Normalgleichung** erreicht.

**Erforderliche Schritte:**

1.  **Darstellung des Systems:** Das urspr√ºngliche √ºberbestimmte Gleichungssystem wird in der Form Ax = B ausgedr√ºckt, wobei A die Koeffizientenmatrix, x der Vektor der unbekannten Variablen und B der Ergebnisvektor ist.
2.  **Umwandlung in die Gau√üsche Normalgleichung:** Um eine l√∂sbare Form zu erhalten, multipliziert man beide Seiten der Gleichung Ax = B von links mit der Transponierten der Matrix A (A·µÄ). Dies f√ºhrt zur Gau√üschen Normalgleichung: **A·µÄAx = A·µÄb**.
3.  **L√∂sung:** Die Gau√üsche Normalgleichung kann dann durch verschiedene Methoden gel√∂st werden, unter anderem durch eine Erweiterung des **‚ÄûFalk-Schemas‚Äú der Matrixmultiplikation**. Die L√∂sung f√ºr x ergibt sich dann durch Multiplikation mit der Inversen von (A·µÄA): **x = (A·µÄA)‚Åª¬π * (A·µÄb)**.

<img src="./img/gleichungssystem.png" width="600" />

**Vorteile:**

* Diese Strategie erm√∂glicht die **n√§herungsweise L√∂sung von √ºberbestimmten Gleichungssystemen**. Dies ist besonders n√ºtzlich in Anwendungsf√§llen wie der Registrierung oder Kamerakalibrierung, wo eine gr√∂√üere Anzahl von Referenzpunkten zur Stabilisierung des Ergebnisses und zur Ann√§herung einer Transformationsmatrix verwendet wird.

**Nachteile:**

* Ungenauigkeiten durch Gleitkommazahl-Arithmetik

---

**0.2 Geben Sie eine (eigene) Definition f√ºr Computer Vision. Welche Disziplinen sind mit welchen Konzepten/Algorithmen involviert?**

Computer Vision ist eine Erweiterung der Bildverarbeitung, die sich mit der komplexen und fortgeschrittenen Bildanalyse befasst. Ihr Ziel ist es, gew√ºnschte Strukturen in Bildern zu erkennen oder zu segmentieren, um ein vollst√§ndiges Bild- und Szenenverst√§ndnis zu erm√∂glichen. Die Eingabedaten k√∂nnen einzelne monokulare Bilder, Videostreams, 3D-Bilder, zus√§tzliche Sensordaten oder 3D-Oberfl√§chenmodelle umfassen, oft auch gro√üe Mengen visueller Daten im Sinne von Big Data.

- **Bildverarbeitung**: Filterung, Bildrekonstruktion, Farbtransformationen, einfache Segmentierungsaufgaben.
- **Optik und Bildgebungssysteme**: Kamerakalibrierung, Verzerrung, 3D-Rekonstruktion.
- **Computergrafik**: Geometrische Modellierung, Formmodelle, Reflexionen, Beleuchtungsmodelle, Testdatengenerierung, 3D-Rekonstruktion, Ergebnisdarstellung.
- **Mustererkennung**: Texturen, SIFT (Scale-Invariant Feature Transform), VSLAM (Visual Simultaneous Localization and Mapping).
- **Robotik**: VSLAM, autonomes Fahren, Produktionsinspektion.
- **K√ºnstliche Intelligenz und Heuristiken**: Merkmalsbasierte Klassifizierung, Registrierung, symbolische Repr√§sentation.
- **Mathematik, Numerik**: Projektive Geometrie, Wahrscheinlichkeiten, Statistik, Signalverarbeitung.

---

**0.3 Was sind klassische Anwendungsgebiete der Computer Vision?**

- **3D-Rekonstruktion aus planaren Bildern**
- **Erkennung (Recognition)**: die verschiedene Formen annehmen kann:
    - Erkennung von Markern und Objekten.
    - Gesichtserkennung, Iris-, Fingerabdruck-, Gesten- und Handschrifterkennung.
    - Erkennung der menschlichen K√∂rperhaltung und Aktivit√§ten.
    - Optische Zeichenerkennung (OCR).
- **Klassifikation/Segmentierung**: um Objekte zu identifizieren und abzugrenzen:
    - Kontursegmentierung von Objekten, um gew√ºnschte Objekte zu extrahieren und zu charakterisieren.
    - Semantische visuelle Klassifikation ganzer Bilder.
    - Komplexere Schlussfolgerungen aus Bilddaten unter Nutzung von K√ºnstlicher Intelligenz (KI). Hierbei wird oft eine semantische Interpretation von Bildern als Voraussetzung ben√∂tigt, beispielsweise durch:
        - **Klassifikation (Classification)**: Das gesamte Bild wird mit einem "Wort" getaggt.
        - **Objektdetektion (Object Detection)**: Zus√§tzlich zur Klassifikation wird der Bereich von Interesse (ROI) um das potenziell verdeckte Schl√ºsselobjekt erkannt, sowohl f√ºr einzelne als auch f√ºr mehrere Objekte.
        - **Instanzsegmentierung (Instance Segmentation)**: Die pr√§zise sichtbare oder verdeckte Pixelregion, die sich auf das klassifizierte Objekt bezieht, wird markiert.
- **Navigation**: insbesondere in autonomen Systemen: Autonomes Fahren an Land, unter Wasser und im Weltraum.
- **Mixed Reality (Erweiterte Realit√§t)**: die reale und computergenerierte Objekte verschmilzt.
- **Synthetische Bilder**: Morphing und computergenerierte Bilder (z.B. in Filmen)

---

**0.4 Welche Bildeigenschaften sind f√ºr die menschliche Wahrnehmung relevant?**

- Oberfl√§chenfarbe
- 3D-Sehen aufgrund der Parallaxe der menschlichen Augen
- Beleuchtung/Schattierung zur Ann√§herung von 3D-Form und Entfernung
- Bildtextur
- Form- und Oberfl√§cheneigenschaften
- A-priori-Wissen √ºber gelernte Objekte, deren Gr√∂√üe und 3D-Struktur
- Stereotypisches Denken als "Kurzschluss" f√ºr das menschliche Erkennungssystem

---

**0.5 Skizzieren Sie Computer Vision mit den relevantesten Verfahren grob als Wissenslandkarte.**

<img src="./img/cv_algorithm.png" width="600" />

---
## 1.Linear Imaging Systems [9]

**1.1 Rotation in 2D: welche zwei M√∂glichkeiten gibt es hierf√ºr? Fertigen Sie eine Skizze an.**

**Rotation des Punkts im fixen Koordinatensystem**
- Rotation von Punkt P aktiv um den Ursprung
- Koordinatensystem bleibt unver√§ndert
- Neue Position des Punkts P' wird mit der Rotationsmatrix berechnet
- Rotationsmatrix:
$$
R(\alpha) =
\begin{pmatrix}
\cos\alpha & -\sin\alpha \\
\sin\alpha & \cos\alpha
\end{pmatrix}
$$

- Transformation:
$$
P' = R(\alpha) \cdot P
$$

**Rotation des Koordinatensystems bei fixem Punkt**
- Statt den Punkt zu drehen, wird das gesamte Koordinatensystem in die verkehrte Richtung (-$\alpha$) rotiert

**Skizzen im Vergleich:** \
Beide f√ºhren zur gleichen neuen Position relativ zum Koordinatensystem:

<img src="./img/linear_2d_rotation.png" width="600" />

---
**1.2: Wie viele Rotationswinkel gibt es in 3D.**
Drei Rotationswinkel, auch drei Rotationsfreiheitsgrade:
- Rotation um x-Achse (Roll)
- Rotation um y-Achse (Pitch)
- Rotation um z-Achse (Yaw)

**Wie gro√ü Matrix f√ºr Translation, Rotation, Skalierung in 3D. Warum 4x4. Wo kommt fehlende 4-te Koordinate in 3D her?**
- Translation: braucht 4√ó4 (nicht mit 3√ó3 darstellbar!)
- Rotation: 4√ó4 (3x3 w√ºrde reichen)
- Skalierung: 4√ó4 (3x3 w√ºrde reichen)

Man braucht 4x4 damit auch Translation als Matrixmultiplikations m√∂glich ist. 4-te Koordinate kommt von der Verwendung von homogenen Koordinaten ($(x,y,z)‚Üí(x,y,z,1)$) -> erlaubt, dass alle affine Transformationen mit einer einzigen 4√ó4-Matrix dargestellt werden k√∂nnen 

<img src="./img/linear_transformation_matrices.png" width="600" />

**Sind affine Transformationen kommutativ? Zeigen Sie anhand eines 2D Beispiels mit Rot/Trans, dass NICHT kommutativ.**

Nein, im Allgemeinen sind sie nicht kommutativ:

<img src="./img/linear_commutative.png" width="600" />

<img src="./img/3.png" width="600" />

---
**1.3 Intrinsische vs. Extrinsische Rotation, d.h. Objekt- bzw. Koordinatensystem-bezogen. Vgl. Vor und Nachteile. Welche Eigenheit gibt es bei der √úberf√ºhrung von extrinsisch nach intrinsisch (Rot-Reihenfolge dreht sich um)**

**Intrinsische Rotation:** Das Objekt rotiert um seine eigenen Achsen, die sich bei jeder Rotation mitdrehen

**Extrinsische Rotation:** Das Objekt wird im fixen Weltkoordinatensystem gedreht. Jede Rotation erfolgt um eine globale Achse, die sich nicht mitdreht

**Eigenheit bei der √úberf√ºhrung:** Wenn man von extrinsisch zu intrinsisch (oder umgekehrt) √ºbergeht, muss man die Reihenfolge der Matrixmultiplikationen umkehren.

| Merkmal     | **Extrinsisch**                           | **Intrinsisch**                                 |
| ----------- | ----------------------------------------- | ----------------------------------------------- |
| Bezug       | Weltkoordinatensystem (fixe Achsen)       | Objektachsen (rotieren mit)                     |
| Anwendung   | Kameraausrichtung, Welt-Tracking          | Robotik, Luftfahrt, Gelenk-Animation            |
| Verst√§ndnis | Mathematisch klarer                       | Visuell besser nachvollziehbar                  |
| Reihenfolge | z ‚Üí y ‚Üí x                                 | x ‚Üí y ‚Üí z                                       |
| Nachteil    | nicht intuitiv bei verketteten Rotationen | Achsen √§ndern sich st√§ndig ‚Üí komplexere Analyse |

---
**1.4 Was bedeutet Kamera-Kalibrierung? Wo liegen die Anwendungsgebiete und welche generellen Einschr√§nkungen k√∂nnen Abbildungssystemen anhaften (distortion, keine echten Farben, usw.)**

Kamera-Kalibrierung ist der Prozess, bei dem man die inneren (intrinsischen), √§u√üeren (extrinsischen) Parameter und optional den Verzerrungskoeffizienten einer Kamera bestimmt, um ein mathematisch korrektes Abbildungsmodell zu erhalten.

> intrinsische Parameter = Brennweite und optisches Zentrum \
> extrinsische Parameter = Rotation und Translation der Kamera zur Welt

**Anwendungsgebiete:**
- Bildvermessung, Bestimmung der Position von Objekten
- 3D-Rekonstruktion
- Tiefenbestimmung
- Forensik 

**Einschr√§nkungen:**
- RGB-Sensoren liefern keine spektral echten Farben
- Bestimmte Tiefe wird fokussiert
- Fertigungsfehler bei Linsen

---
**1.5 Warum kommt es bei der Camera Obscura zu Unsch√§rfe? Fertigen Sie eine Skizze an und diskutieren Sie wovon es abh√§ngt, dass ein Bild mittels Camera Obscura vergr√∂√üert oder verkleinert wird. Konnex zu adaptiven Filtern zwecks Korrektur**
- Die Camera Obscura verwendet ein Loch (kein Linsensystem), durch das Licht auf eine Projektionsfl√§che f√§llt. Jeder Bildpunkt entsteht durch Lichtstrahlen, die von einem Punkt in der Szene durch das Loch auf der Fl√§che treffen.
- Ist das Loch zu gro√ü, gelangen mehrere Lichtstrahlen von leicht unterschiedlichen Richtungen durch das Loch auf denselben Bildpunkt -> **Unsch√§rfe**
- Gr√∂√üe der √ñffnung:
    - kleiner ‚Üí sch√§rferes Bild, aber dunkler (weniger Licht)
    - gr√∂√üer ‚Üí helleres, aber unsch√§rferes Bild

- Abstand von Objekt und Projektionsfl√§che:
    - Gr√∂√üerer Abstand ‚Üí gr√∂√üere Projektion (Bild wird vergr√∂√üert), aber auch potenziell unsch√§rfer
    - N√§her am Loch ‚Üí kleineres, sch√§rferes Bild

Skizze:

<img src="./img/linear_obscura.png" width="600" />

**Konnex zu adaptiven Filtern zwecks Korrektur:**
- Unsch√§rfe kann man durch adaptive Filter bereinigen (z.B. Anisotrope Diffusion)

---
**1.6 Welche intrinsischen und extrinsischen Parameter als Matrizeninhalte sind bei der Kamera-Kalibrierung zu l√∂sen? Beschreiben Sie jeden der Parameter. Warum gibt es 2 Parameter f√ºr die Fokusl√§nge (x,y)? Durch welche weiteren Parameter kann eine Verzerrungskorrektur erfolgen?**

**Instrinsische Parameter**
$$
\begin{bmatrix}
\phi_x & \gamma & \delta_x \\
0 & \phi_y & \delta_y \\
0 & 0 & 1
\end{bmatrix}
$$

- $\phi_x$ - horizontale Brennweite (Fokusl√§nge)
- $\phi_y$ - vertikale Brennweite (Fokusl√§nge)
- $\gamma$ - Scherung zwischen x- und y-Achse
- $\delta_x$ - x-Koordinate des Bildzentrums
- $\delta_y$ - y-Koordinate des Bildzentrums
- => es gibt zwei Fokusl√§ngen, weil die physikalische Gr√∂√üe der Pixel in x- und y-Richtung nicht gleich sein muss.

**Extrinsische Parameter**
$$
\begin{bmatrix}
\omega_{11} & \omega_{12} & \omega_{13} & \tau_{x} \\
\omega_{21} & \omega_{22} & \omega_{23} & \tau_{y} \\
\omega_{31} & \omega_{32} & \omega_{33} & \tau_{z} \\
\end{bmatrix}
$$

- $\omega$ - 3x3 Rotationsmatrix - Orientierung der Kamera im Raum
- $\tau$ - 3x1 Translationsmatrix - Position der Kamera

**Verzerrungskorrektur**
$$
x‚Ä≤=x‚ãÖ(1+\beta_1‚Äãr^2+\beta_2‚Äãr^4) \\
y‚Ä≤=y‚ãÖ(1+\beta_1‚Äãr^2+\beta_2‚Äãr^4)
$$

- $x,y$ - urspr√ºngliche Koordinaten
- $x',y'$ - verzerrte Koordinaten
- $r$ - Radius
- $\beta$ - radiale Verzerrungskoeffizienten

---
**1.7 Welche grundlegenden 3 geometrischen Probleme k√∂nnen mittels Kamera Kalibrierung gel√∂st werden? Erl√§utern Sie ev. mit Skizze.**

- Bestimmung der extrinsischen Parameter
- Bestimmung der intrinsische Parameter

<img src="./img/linear_parameter.png" width="600" />

- 3D-Rekonstruktion aus mehreren Ansichten

<img src="./img/linear_3d_reconstruction.png" width="600" />

---
**1.8 Beschreiben Sie den Verfahrensablauf bei der Kalibrierung einer Kamera (Schachbrett-Muster).**
- Ziel: Bestimmung der intrinsischen und extrinsischen Kameraparameter durch Beobachtung eines bekannten 2D- oder 3D-Kalibriermusters, z.B. eines Schachbretts
1. Kalibriermuster aufnehmen
    - Schachbrettmuster mit 7√ó6 Gitterpunkte
    - Muster muss aus mehreren Blickwinkeln aufgenommen werden
2. Referenzpunkte definieren
    - Weil Muster 2D ist -> z = 0
    - Die Punkte werden definiert: (0,0),(1,0),... oder in echten Einheiten: (0,0),(0,50),...
3. Bildpunkte extrahieren
    - Detektion der zugeh√∂rigen Bildpunkte‚Äã im aufgenommenen Bild
    - z.B. √ºber automatische Checkerboard-Erkennung
4. Gleichungssystem aufstellen
    - Jedes Punktpaar liefert zwei Gleichungen
    - Es gibt insgesamt 12 Unbekannte (Parameter in der Projektionsmatrix)
    - Mindestens 6 Punktpaare sind n√∂tig ‚Äì mehr Punkte verbessern die Genauigkeit (‚Üí Overdetermined system)
5. Optimierung der Parameter
    - Intrinsische und extrinsische Parameter werden gesch√§tzt & optimiert
6. Verzerrungskorrektur anwenden
    - Bild kann nun entzerrt werden

---
**1.9 Erl√§utern sie grob, wie die Gleichungen zur Kalibrierung einer Kamera definiert und gel√∂st werden k√∂nnen. Ev. mit Skizze.**

```math
\lambda
\begin{bmatrix}
x \\
y \\
1
\end{bmatrix}
=
\begin{bmatrix}
\phi_x & \gamma & \delta_x & 0 \\
0 & \phi_y & \delta_y & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}
\cdot
\begin{bmatrix}
\omega_{11} & \omega_{12} & \omega_{13} & \tau_{x} \\
\omega_{21} & \omega_{22} & \omega_{23} & \tau_{y} \\
\omega_{31} & \omega_{32} & \omega_{33} & \tau_{z} \\
0 & 0 & 0 & 1
\end{bmatrix}
\cdot
\begin{bmatrix}
u \\
v \\
w \\
1
\end{bmatrix}
```

- $\lambda$ - Skalierungsfaktor
- $\begin{bmatrix}
    x \\
    y \\
    1
   \end{bmatrix}$ - Bildkoordinaten
- $\begin{bmatrix}
    \phi_x & \gamma & \delta_x & 0 \\
    0 & \phi_y & \delta_y & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1
   \end{bmatrix}$ - intrinsisch
- $\begin{bmatrix}
    \omega_{11} & \omega_{12} & \omega_{13} & \tau_{x} \\
    \omega_{21} & \omega_{22} & \omega_{23} & \tau_{y} \\
    \omega_{31} & \omega_{32} & \omega_{33} & \tau_{z} \\
    0 & 0 & 0 & 1
   \end{bmatrix}$ - extrinsisch
- $\begin{bmatrix}
    u \\
    v \\
    w \\
    1
  \end{bmatrix}$ - Weltkoordinaten

**Wie kann die Gleichung gel√∂st werden?**
- Annahme: Die intrinsischen Parameter m√ºssen nicht berechnet werden, siehe Skizze:

<img src="./img/linear_calibration.png" width="400" />

1. F√ºr jeden Punkt im Bild werden zwei Gleichungen definiert (eine f√ºr x, eine f√ºr y)
2. Dabei haben wir 12 unbekannte Parameter (extrinsisch) => zum L√∂sen kann das SVD (Singular Value Decomposition) angewendet werden  

---
## 2. Segmentation and Classification [8]

**2.1 Differenzieren Sie die Begriffe Klassifikation, Segmentierung und Lokalisierung f√ºr ein BSP mit 1-N Objekten.**

**Klassifikation.**

- Definition: Bestimmung der Objektklasse ohne Ortsangabe
- Beispiel: Das System erkennt nur "CAT" (Katze) im Bild
- Ausgabe: Klassenbezeichnung (z.B. "Katze", "Hund", "Ente")
- Anwendung: Bei einem Objekt im Bild

**Klassifikation + Lokalisierung**

- Definition: Bestimmung der Objektklasse UND deren Position
- Beispiel: Das System erkennt "CAT" und umrahmt die Katze mit einem roten Bounding Box
- Ausgabe: Klassenbezeichnung + Koordinaten der Begrenzungsbox
- Anwendung: Bei einem Objekt im Bild

**Object Detection**

- Definition: Erkennung und Lokalisierung multipler Objekte
- Beispiel: Das System erkennt und lokalisiert "CAT, DOG, DUCK" mit verschiedenfarbigen Bounding Boxes
- Ausgabe: Multiple Klassenbezeichnungen + jeweilige Begrenzungsboxen
- Anwendung: Bei 1-N Objekten im Bild

**Instance Segmentation**

- Definition: Pixelgenaue Segmentierung jeder Objektinstanz
- Beispiel: Jedes Objekt (Katze, Hund, Ente) wird pixelgenau in unterschiedlichen Farben markiert
- Ausgabe: Pixelmaske f√ºr jede Objektinstanz + Klassenbezeichnung
- Anwendung: Bei 1-N Objekten mit pr√§ziser Formerfassung

<img src="./img/klassifikation.png" width="600" />

_Why tf are they so damn cute?_

Die Komplexit√§t steigt von links nach rechts: Klassifikation ‚Üí Lokalisierung ‚Üí Detection ‚Üí Instance Segmentation.

---

**2.2 Wie kann man mittels MeanShift eine Vorsegmentierung bewirken, wie mittels KMeans Clustering + Quantisierung/Region Labelling, wie mittels Anisotroper Diffusion + Quantisierung/Region Labelling? Vergleichen Sie und erl√§utern Sie kurz.**

**Mean Shift**
- Funktionsweise:
    - Pixels werden als mobile Punkte in einem 3D-RGB-Farbraum interpretiert
    - Jeder Punkt bewegt sich iterativ zum lokalen Maximum der Kernel Density Estimation (KDE)
    - Verwendet Gaussian Kernel mit spezifischem Radius zur lokalen Durchschnittsberechnung
    - Partikel konvergieren zu lokalen Clusterzentren
- Vorteile:
    - Parameter-frei (keine Vorab-Clusterzahl erforderlich)
    - Sehr robust f√ºr homogene Farbregionen
    - Kann direkt aus Histogramm berechnet werden
- Nachteile:
    - Rechenintensiv durch iterative Bewegung aller Punkte

**K-Means Clustering + Quantisierung/Region Labelling**
- Funktionsweise:
    - Clustering basiert auf Intensit√§ts-√Ñhnlichkeit
    - Iterative Neuberechnung der Clusterzentren (Centroids)
    - Pixel werden dem n√§chstgelegenen Centroid zugeordnet
    - Anschlie√üend Region Labelling f√ºr r√§umlich zusammenh√§ngende Bereiche
- Vorteile:
    - Einfach und schnell implementierbar
    - Gute Ergebnisse bei homogener Beleuchtung
- Nachteile:
    - Inhomogene Kontraste und Beleuchtung k√∂nnen zu schlechten Segmentierungen f√ºhren
    - Clusterzahl n muss vorab definiert werden
    - Region Labelling als zus√§tzlicher Post-Processing-Schritt erforderlich

<img src="./img/kmeans.png" width="600" />

> Obwohl die Clusteranzahl bei K-Means vorab definiert wird, k√∂nnen nach dem Region Labelling mehr Regionen als Cluster entstehen, da nicht alle Pixel innerhalb eines Clusters √∂rtlich zusammenh√§ngen m√ºssen.

<img src="./img/kmeans_result.png" width="600" />


**Anisotrope Diffusion + Quantisierung/Region Labelling**
- Funktionsweise:
    - Anisotrope Diffusion _(wird bei 3.1 erkl√§rt)_ mit vielen Iterationen gl√§ttet Bereiche innerhalb von Gradientengrenzen
    - Quantisierung auf ausreichende Granularit√§t (z.B. [0;15])
    - Region Labelling f√ºr zusammenh√§ngende Bereiche
- Vorteile:
    - Erh√§lt wichtige Objektgrenzen (Gradientenerhaltung)
- Nachteile:
    - Rechenintensiv durch viele Diffusionsiterationen
    - Mehrere Verarbeitungsschritte erforderlich

<img src="./img/anisotropic_diff.png" width="600" />


**Vergleichstabelle:**

| Kriterium | Mean Shift | K-Means | Anisotrope Diffusion |
|-----------|------------|---------|---------------------|
| **Parameter** | Parameter-frei | Clusterzahl $n$ erforderlich | Iterationszahl |
| **Geschwindigkeit** | Langsam | Schnell | Langsam |
| **Beleuchtungsrobustheit** | Gut | Schlecht | Sehr gut |
| **Post-Processing** | Nope | Region Labelling | Quantisierung + Region Labelling |

---

**2.3 Erl√§utern Sie kurz Graph Cut / Grab Cut. BG und FG-Wurzeln im Graph, Kantengewichte und Schnitt. Wie kann mittels Benutzerinteraktion ein verbessertes Ergebnis bewirkt werden?**

Grab Cut ist eine Weiterentwicklung von Graph Cut f√ºr die interaktive Vordergrund-/Hintergrund-Segmentierung mit minimaler Benutzerinteraktion.

**Grundprinzip:** 

- Farben von Vordergrund (FG) und Hintergrund (BG) werden als GMM (Gaussian Mixture Model) modelliert
- Jeder Pixel erh√§lt einen Œ±-Wert in [0;1] f√ºr die Zugeh√∂rigkeit zu BG (Œ±=0) oder FG (Œ±=1)
- Das GMM wird iterativ basierend auf den zugewiesenen Pixeln aktualisiert

**Graph-Struktur**
- BG und FG-Wurzeln:
    - Source: Verbunden mit garantierten Vordergrund-Pixeln
    - Sink: Verbunden mit garantierten Hintergrund-Pixeln
    - Alle Pixel sind als Knoten im Graph repr√§sentiert
- Kantengewichte (Energiefunktion kombiniert zwei Komponenten):
    - $E = E_{color} + E_{coherence}$
    - $E_{color}$: GMM-basierte Farb√§hnlichkeit
    - $E_{coherence}$: Lokale Nachbarschaft und Gradientenseparation

**Schnitt:**
- Der Graph wird so geschnitten, dass FG- und BG-Regionen getrennt werden
- Dabei werden Pixel-Verbindungen mit der h√∂chsten "Distanz" (Gradienten, Farbunterschiede) entfernt
- Das Ergebnis minimiert die Gesamtenergie

**Iterativer Prozess:**
Das GMM wird iterativ geupdated basierend auf den neu zugewiesenen Pixeln, und der Graph-Schnitt wird neu durchgef√ºhrt bis Konvergenz erreicht ist ‚Üí das Prinzip der "iterative energy minimization".

**Benutzerinteraktion f√ºr bessere Ergebnisse**

- Initiale ROI-Definition: Benutzer definiert groben Bereich um das Objekt
- Seed-Korrektur: Bei unzureichenden Ergebnissen k√∂nnen zus√§tzliche Seeds gesetzt werden:
    - FG-Seeds (Wert 255): Markierung sicherer Vordergrund-Bereiche
    - BG-Seeds (Wert 0): Markierung sicherer Hintergrund-Bereiche
- Iterative Verbesserung: Diese Seeds dienen als "Ground Truth" und verfeinern das GMM

<img src="./img/grab_cut.png" width="600" />

<img src="./img/grab_cut_sample.png" width="200" />

--- 

**2.4 Diskutieren Sie bei der OCR die technischen H√ºrden f√ºr a) die Erkennung von Text-Passagen und b) f√ºr die eigentliche Analyse der Buchstaben. Nennen Sie die daf√ºr jeweils notwendigen verfahren.**

**Technische H√ºrden bei der Erkennung von Text-Passagen**

- Komplexe Bildanalyse: Text kann in verschiedenen Gr√∂√üen, Positionen und Orientierungen im Bild auftreten
- Variabilit√§t: Unterschiedliche Schriftarten, Gr√∂√üen und Ausrichtungen erschweren die Detektion
- Hintergrund-Separation: Trennung von Text und Hintergrund bei komplexen Bildern
- Scan-Qualit√§t: Verzerrungen, Rauschen und Unsch√§rfe bei gescannten Dokumenten

**dazugeh√∂rige Verfahren:**

- EAST (Efficient and Accurate Scene Text Detector)
    - Nutzt rekurrente neuronale Netzwerke
    - Erstellt eine Pixel-Score-Map f√ºr lokale Zeichen-Wahrscheinlichkeit
    - Gruppiert wahrscheinliche Zeichen-Pixel zu rechteckigen Bereichen

- Region Labelling Strategien
    - Identifikation zusammenh√§ngender Textbereiche
    - Segmentierung in einzelne Textzeilen

- "Fire-through" Methode
    - Bei nicht √ºberlappenden Zeilen anwendbar
    - Horizontale/vertikale Projektion zur Zeilentrennung

**Technische H√ºrden bei der Buchstaben-Analyse**

- √Ñhnliche Buchstaben: Schwierige Unterscheidung zwischen √§hnlichen Zeichen (z.B. "b" und "d")
- Qualit√§tsverluste: Sensor-Rauschen, Unsch√§rfe, Beleuchtungsunterschiede
- Verzerrungen: Affine Transformationen durch schr√§ge Aufnahmen
- Diskretisierung: Anti-Aliasing-Effekte bei der Digitalisierung

**dazugeh√∂rige Verfahren:**

- Merkmals-Extraktion
    - Geometrische Features: Anzahl Pixel, Breite/H√∂he, Zentroid-Abst√§nde
    - Topologische Features: Anzahl Inseln, Verzweigungspunkte
    - Korrelationskoeffizient-Berechnung f√ºr √Ñhnlichkeitsvergleich

- Machine Learning Ans√§tze
    - SVM (Support Vector Machines)
    - Neuronale Netzwerke
    - Deep Learning mit RNNs (besonders LSTM f√ºr Kontextanalyse)

- Moderne Deep Learning Architekturen
    - Convolutional Layers: F√ºr Vorverarbeitung und Feature-Extraktion
    - Auxiliary Stage: Mehrere Schichten f√ºr Vektorverarbeitung
    - Recurrent Stage: GRU-Layer (√§hnlich LSTM) f√ºr variable Eingabel√§ngen

- Qualit√§tsverbesserung
    - Sprachmodelle: Plausibilit√§tspr√ºfung durch bekannte W√∂rter
    - Kontext-Analyse: Ber√ºcksichtigung vorheriger/nachfolgender Buchstaben
    - Template-Matching: Bei bekannten Schriftarten

--- 

**2.5 Nennen Sie einige relevante Features aus dem Bereich Textur, Geometrie und Transformation. Welche dieser Features k√∂nnen auch im Bereich OCR zur Analyse von bin√§ren Buchstaben innerhalb einer region of interest (ROI) verwendet werden? Inwieweit beinhalten klassische Segmentierungsverfahren wie Interval-Threshold oder Region-Growing ebenfalls Features?**

**Allgemein**

- Textur-Features
    - LBP (Local Binary Patterns): Lokale Nachbarschaftsanalyse in 3x3 Bl√∂cken
    - Haar-Features: Verschiedene Konvolutionskerne zur Detektion spezifischer Charakteristika
- Geometrie-Features
    - F‚ÇÅ: Anzahl der Pixel
    - F‚ÇÇ: Ausdehnung in x-Richtung (max. Breite)
    - F‚ÇÉ: Ausdehnung in y-Richtung (max. H√∂he)
    - F‚ÇÑ: Durchschnittlicher Abstand vom Zentroid
    - F‚ÇÖ: Minimaler Abstand vom Zentroid
    - F‚ÇÜ: Maximaler Abstand vom Zentroid
    - F‚Çá: Zirkularit√§t
    - F‚Çà: Relative x-Position des Zentroids in der Bounding Box
    - F‚Çâ: Relative y-Position des Zentroids in der Bounding Box
- Transformation-Features
    - Affine Transformationen: Rotation, Skalierung, Translation
    - Delaunay-Triangulation: Bei Gesichtserkennung f√ºr invariante Topographie

**Features f√ºr OCR-Buchstaben-Analyse**

- Alle oben genannten F‚ÇÅ-F‚Çâ
- Anzahl der Inseln
- Anzahl der Verzweigungspunkte
- Mittlere Richtungs√§nderung

**Features in klassischen Segmentierungsverfahren**

Klassische Segmentierungsverfahren beinhalten Features nur in sehr geringem Umfang:
- **Interval-Thresholding:**
    - Nutzt Intensit√§tswerte als Feature
    - Segmentiert basierend auf Schwellenwerten (z.B. Grauwert-Histogramm)
- **Region-Growing:**
    - Basiert ebenfalls prim√§r auf Intensit√§tswerten (Tmin, Tmax)
    - Zus√§tzlich: r√§umliche Nachbarschaftsinformation
    - Implizite geometrische Komponente

--- 

**2.6 Erl√§utern Sie das Vorgehen, wenn man OCR ‚Äûvon 0 weg‚Äú eigenst√§ndig umsetzen m√∂chte auf Basis von vorsegmentierten Regionen und Featureanalyse f√ºr die Klassifikation (vgl. √úbung).**

1. Bildvorverarbeitung und Binarisierung 
    - mit Intervall-Thresholding
2. Textsegmentierung
    - "Fire-Through" Methode: Horizontale Analyse des gesamten Bildes
    - Erkennung von Textzeilen durch Detektion von Zeilen mit Vordergrundpixeln
3. Zeichentrennung
    - Vertikale "Fire-Through" Technik innerhalb jeder erkannten Zeile
    - Suche nach leeren Spalten zwischen Zeichen als Trennpunkte
4. (Optional) Zeichenregion-Optimierung (Region Shrinking)
    - Anpassung der Bounding Box auf die tats√§chlichen Zeichengrenzen
    - Vorteil: Genauere Feature-Berechnung, bessere Aspekt-Verh√§ltnisse
5.  Feature-Extraktion
    - Berechnung von Durchschnittswerten √ºber alle Zeichenregionen
6.  Referenzzeichen-Auswahl
    - Feature-Vektor des ausgew√§hlten Zeichens wird als Referenz verwendet
7. Vergleichsprozess
    - Vergleich der extrahierten Features mit dem Referenzzeichen
    - Berechnung des Korrelationskoeffizienten zwischen den Vektoren
    - Schwellwert-basierte Entscheidung (z.B. 0.999 f√ºr hohe Genauigkeit)
8. Ergebnisvisualisierung
---

**2.7 Was sind Haar Cascades? Wie werden sie trainiert und angewandt? Inwieweit ist das Konzept skalierungsinvariant? F√ºhren Sie √ºber die Bedeutung der Haar Cascade im Bereich der _??? (Hier hat Gerald ein kleines Schlagerl gehabt)_**

Haar Cascades sind ein mehrstufiges Klassifikationssystem f√ºr die Gesichtserkennung. Das System nutzt Haar-√§hnliche Features (spezielle Convolution-Kernel) zur Erkennung charakteristischer Gesichtsmerkmale wie Augen, Nase und Mund. Haar-√§hnliche Features sind spezielle rechteckige Convolution-Kernel (Filter), die Helligkeitsunterschiede zwischen benachbarten Bildregionen detektieren - beispielsweise dunkle Augenbereiche neben helleren Wangenbereichen oder der dunkle Bereich zwischen den Augen.

**Training von Haar Cascades:**
Trainingsdaten:
- Positive Beispiele: Viele Trainingsbilder mit Gesichtern
- Negative Beispiele: Bilder mit verschiedenen anderen Objekten (ohne Gesichter)

Trainingsverfahren:
1. Haar Features: Verschiedene Haar-Kernel werden in unterschiedlichen Ma√üst√§ben, Orientierungen und Positionen innerhalb der Bild-ROIs angewendet
2. AdaBoost-Algorithmus: Wird verwendet, um die wichtigsten Features zu identifizieren und eine Cascade-Struktur aufzubauen
3. Feature-Auswahl: Das System pr√ºft zuerst die signifikantesten Features und evaluiert weitere nur dann, wenn ein Gesicht noch wahrscheinlich ist

**Anwendung**

Sliding Window Ansatz
- Das gesamte Bild wird in Sub-Image ROIs aufgeteilt (Sliding Windows verschiedener Gr√∂√üen)
- Filter-Pyramide f√ºr verschiedene Skalierungen

Cascade-Verarbeitung
- Mehrstufige Klassifikation: Nur wenn Features einen vordefinierten Schwellenwert erreichen, wird der Prozess fortgesetzt/verfeinert
- Effizienzsteigerung: Reduziert den Rechenaufwand erheblich, da nicht alle Features f√ºr jeden Bildbereich berechnet werden m√ºssen

Non-Maximum Suppression
- Mehrere √ºberlappende Gesichtskandidaten werden zu einem finalen Ergebnis zusammengefasst

**Skalierungsinvarianz**

Das Haar Cascade-Konzept ist skalierungsinvariant durch:
1. Filter-Pyramide: Anwendung der Haar Features in verschiedenen Ma√üst√§ben
2. Multi-Scale Detection: Das System kann Gesichter in unterschiedlichen Gr√∂√üen erkennen
3. Skalierbare Kernel: Die Haar Features werden in verschiedenen Gr√∂√üen angewendet

**Bedeutung der Haar Cascade**

_(Achtung Antworten nur gesch√§tzt, weil Frage unvollst√§ndig -> immer dieser Gerald üôÇ‚Äç‚ÜîÔ∏è)_

+ Vorteile:
    - Hohe Genauigkeit und niedrige False-Positive-Rate
    - Robuste Erkennung unter verschiedenen Bedingungen
    - Etablierter Standard in der Computer Vision

\- Nachteile:
    - Langsamer als LBP (local Binary Patterns)
    - Weniger robust gegen Verdeckungen
    - Weniger genau bei dunklen Gesichtern


> Haar Cascades waren wegweisend f√ºr die automatische Gesichtserkennung und bildeten lange Zeit den Goldstandard f√ºr Echtzeit-Gesichtsdetektion. Sie sind bis heute in OpenCV implementiert und werden als Referenz f√ºr Vergleiche mit modernen Deep Learning-Ans√§tzen verwendet.
> Obwohl moderne Deep Learning-Methoden heute bessere Ergebnisse erzielen, bleiben Haar Cascades ein wichtiges Grundlagenkonzept.
---

**2.8. Nennen und Diskutieren Sie alternative Ans√§tze zur Gesichtserkennung (historische Verfahren bis hin zu aktuellen Strategien) und charakterisieren Sie dabei jeweils die Vor- und Nachteile.**

**Historische Verfahren**
1. Geometrische Merkmale (Kanade 1973, Brunelli & Poggio 1992)

Ansatz:
- 16 Landmarken und 40 Features (Kanade) bzw. 22 Landmarken (Brunelli & Poggio)
- Berechnung von Winkeln und Seitenverh√§ltnissen zwischen Merkmalen
- Vergleich √ºber euklidische Distanz

Vorteile:
- Robust bei variierender Beleuchtung und kleinen Gesichtsausdrucks√§nderungen

Nachteile:
- Automatische Landmarken-Detektion schwierig und instabil
- Nur 75% Erkennungsrate bei 20 Personen
- Zu wenige Informationen f√ºr robuste Diskriminierung

**Traditionelle Computer Vision**
2. Haar Cascade (Viola & Jones 2001)

Ansatz:
- Haar-Features verschiedener Gr√∂√üe, Orientierung und Position
- AdaBoost-Algorithmus mit mehrstufiger Klassifikation (Cascade)

Vorteile:
- Hohe Erkennungsgenauigkeit
- Niedrige Falsch-Positiv-Rate

Nachteile:
- Rechnerisch komplex und langsam
- L√§ngere Trainingszeit
- Weniger genau bei dunklen Gesichtern
- Eingeschr√§nkt bei schwierigen Beleuchtungsbedingungen
- Weniger robust gegen Verdeckungen

3. Local Binary Patterns (LBP) (Ahonen et al. 2006)

Ansatz:
- Analyse lokaler 3x3 Pixelbl√∂cke
- Encoding in Histogramme mit 203-bin Vektor

Vorteile:
- Rechnerisch einfach und schnell
- K√ºrzere Trainingszeit
- Robust gegen√ºber lokalen Beleuchtungs√§nderungen
- Robust gegen Verdeckungen

Nachteile:
- Geringere Genauigkeit
- H√∂here Falsch-Positiv-Rate

4. Active Appearance Models (Edwards et al. 1998)

Ansatz:
- Statistische Form- und Texturmodelle
- Harmonisierung von Gesichtsausdr√ºcken vor Vergleich

Vorteile:
- Ber√ºcksichtigt verschiedene Gesichtsausdr√ºcke
- Vergleich von Form und Textur

**Aktuelle Deep Learning Ans√§tze**
5. Deep Learning (FaceNet - Schroff et al. 2015)

Ansatz:
- 22-schichtiges Netzwerk mit 140+ Millionen Parametern
- 128-dimensionale L2-normalisierte Repr√§sentation
- Training mit LFW Dataset (Millionen von Bildern)

Vorteile:
- 88% Erkennungsrate (Einzelbilder)
- 95% bei Videosequenzen (erste 100 Frames)
- Automatische Merkmalsextraktion

Nachteile:
- Hoher Rechenaufwand
- Gro√üe Trainingsmengen erforderlich

>Entwicklungstrend: Von 75% (Kanade) zu 96-98% Erkennungsrate bei modernen Ans√§tzen, mit Trend von manuell definierten zu automatisch gelernten Merkmalen.

 
---
## 3. Image Restauration [5]

**3.1 Erl√§utern Sie die Idee der Anisotropen Diffusion mit eigenen Worten. Was bedeutet die lokale Diffusionszahl ? Was bedeutet K? Wird c f√ºr jedes Pixel individuell berechnet?  Wird K f√ºr jedes Pixel individuell berechnet?  Wie viele Gradientenrichtungen sind f√ºr jedes Pixel zu betrachten?**

Anisotrope Diffusion ist ein iteratives Verfahren zur Bildrestauration, das auf einem physikalischen Modell basiert. Die Grundidee ist, dass Pixel wie Partikel behandelt werden, die ihre Intensit√§tswerte √ºber die Zeit austauschen - √§hnlich wie bei der W√§rmediffusion.

>Entscheidend ist jedoch, dass dieser Austausch **nicht** gleichm√§√üig in alle Richtungen erfolgt (daher "anisotrop"), sondern durch Kanten und Gradienten im Bild gehemmt wird.

Das Ziel ist es, homogene Bildbereiche zu gl√§tten (Rauschreduktion), w√§hrend wichtige Strukturen wie Kanten erhalten bleiben.

Die lokale Diffusionszahl $c(x,y)$ steuert, wie stark die Diffusion an jedem Bildpunkt stattfindet:

- $c = 1$: Ungehemmte isotrope Diffusion (wie bei einem normalem Tiefpass-Filter)
- $c < 1$: Reduzierte Diffusion an Kanten und Gradienten

Die Konstante $K$ ist ein globaler Parameter, der die Empfindlichkeit des Filters steuert:

- $K$ bestimmt den Schwellwert, ab welcher Gradientenst√§rke die Diffusion gehemmt wird
- Kleine $K$-Werte: Bereits schwache Gradienten hemmen die Diffusion ‚Üí starke Kantenerhaltung
- Gro√üe $K$-Werte: Nur starke Gradienten hemmen die Diffusion ‚Üí mehr Gl√§ttung

Individuelle Berechnung pro Pixel:

- $c(x,y)$ wird f√ºr **jedes** Pixel individuell berechnet, basierend auf dem lokalen Gradienten an dieser Position
- $K$ wird **nicht** f√ºr jedes Pixel individuell berechnet - es ist ein globaler Parameter, der f√ºr das gesamte Bild konstant bleibt

Anzahl der Gradientenrichtungen:

- 4 Hauptrichtungen: Nord, S√ºd, Ost, West (Distanz = $1$)
- 4 Diagonalrichtungen: Nordost, Nordwest, S√ºdost, S√ºdwest (Distanz = $\sqrt{2}$)

Algorithmus-Ablauf in Prosa:

1. Filterkernel f√ºr 8 Richtungen (Nord, Nordwest, etc.) werden definiert
2. Hauptschleife (f√ºr jede Iteration):
    1. **Gradientenberechnung**: Das Bild wird mit allen 8 Richtungskerneln gefaltet ‚Üí 8 Gradientenbilder
    2. **Diffusionskoeffizienten**: F√ºr jedes Pixel und jede Richtung wird $c = e^{-(gradient/K)¬≤}$ berechnet
    3. **Bildupdate**: Jeder Pixelwert wird um den gewichteten Durchschnitt aller 8 Richtungen erg√§nzt
3. Ergebnis: Gegl√§ttetes Bild mit erhaltenen Kanten.

---

**3.2 Was bedeutet ‚Äûimage degradation‚Äú. Filterung im Ortsraum vs. Frequenzraum. Sehen Frequenz-Spektren von nat√ºrlichen Bildern √§hnlich aus? Wie kann man das nun zur Beseitigung von St√∂rsignalen n√ºtzen? Warum muss bei der ‚Äûdeconvolution‚Äú im Falle von Rauschen der hohe Frequenzbereich abgeschw√§cht werden (Skizze).**

Image degradation bezeichnet die Verschlechterung der Bildqualit√§t durch verschiedene Faktoren:
- Motion blur (Bewegungsunsch√§rfe)
- Optische Unsch√§rfe durch Linsen
- Niedrige Aufl√∂sung
- R√§umliche Quantisierung (diskrete Pixel, Partial Volume Effect)
- Additives Rauschen verschiedener Art

Die mathematische Modellierung ist simpel: $g(x,y) = f(x,y) * h(x,y) + n(x,y)$
$f$ ... urspr√ºngliches Bild
$h$ ... Degradationskernel
$n$ ... Rauschen
$g$ ... degradiertes Bild

Filterung im Ortsraum vs. Frequenzraum:

- Ortsraum (Spatial Domain):
    - Degradation durch Faltung mit einem Kernel: $g(x,y) = f(x,y) * h(x,y)$
    - Direkte Pixelmanipulation
- Frequenzraum (Frequency Domain):
    - Faltung im Ortsraum entspricht Multiplikation im Frequenzraum: $G(u,v) = F(u,v) \cdot H(u,v)$
    - Transformation via FFT erforderlich und wieder zur√ºck mit IFFT

Nat√ºrliche Bilder zeigen √§hnliche Spektren im Frequenzbereich
- Hohe Amplituden bei niedrigen Frequenzen
- Niedrige Amplituden bei hohen Frequenzen
- Diese Eigenschaft kann zur Approximation der Degradationsmodelle genutzt werden

Die oben genannten Erkenntnisse kann man zur Beseitigung von St√∂rsignalen nutzen:

- Modellierung der Degradation durch bekannte Kernel (z.B. Gauss f√ºr optische Unsch√§rfe, Sinc f√ºr Motion Blur)
- Dekonvolution zur Wiederherstellung: $FÃÇ(u,v) = G(u,v)/H(u,v)$
- Wiener-Filter f√ºr rauschbehaftete Bilder
- Richardson-Lucy Dekonvolution f√ºr iterative Verbesserung

Warum muss bei der ‚Äûdeconvolution‚Äú im Falle von Rauschen der hohe Frequenzbereich abgeschw√§cht werden (Skizze).

$FÃÇ(u,v) = G(u,v)/H(u,v)$
Mathematische Ursache:

- Degradationsfunktionen $H(u,v)$ haben Tiefpass-Charakter
- $H(u,v)$ wird bei hohen Frequenzen sehr klein oder null
- Division durch sehr kleine Werte verst√§rkt das Rauschen extrem
- $N(u,v)$ ist das additive Rauschspektrum

<img src="./img/high_frequency_problem.png" width="600" />

- X-Achse ist die Frequenz ($H(u,v)$ hat Tiefpass-Charakter)
- Y-Achse ist Amplitude/Verst√§rkungsfaktor
- Gr√ºn ist das Rauschen
- Die blaue Linie kennzeichnet das Einf√ºhren einer D√§mpfungsfunktion,
z.Bsp. Butterworth Dampening

Praktische Konsequenz:
Ohne Abschw√§chung wird das rekonstruierte Bild v√∂llig verrauscht, da das Rauschen bei hohen Frequenzen extrem verst√§rkt wird. 




---

**3.3 Die Wiener-Deconvolution kann mittels Wiener-Filterkern erfolgen, wobei K welche Bedeutung zukommt? ( Absch√§tzung des Signal-Rausch-Verh√§ltnis). Wie kommt man zu K? Wie vereinfacht sich die Wiener-Deconvolution, wenn kein Rauschen im Bild vorhanden ist.**

$K$ repr√§sentiert eine **empirische** Konstante zur Absch√§tzung des Signal-Rausch-Verh√§ltnisses. 

$K$ ermittelt man durch:
- Empirische Sch√§tzung: $K$ wird durch Erfahrungswerte und Experimente gesch√§tzt
- A-priori Spezifikation: Der Wert muss vorab festgelegt werden basierend auf:
    - Erwarteter Rauschst√§rke im Bild
    - Charakteristika des Signals
    - Empirischen Tests mit √§hnlichen Bilddaten

Wenn **kein** Rauschen im Bild vorhanden ist, gilt K ‚Üí 0, und die Wiener-Deconvolution vereinfacht sich zur gew√∂hnlichen inversen Filterung.

$F`(u,v)=\frac{F(u,v)}{H'(u,v)}$

$F'$ ... Rekonstruiertes Bild (im Frequenzbereich)
$H'$ ... Gesch√§tzter Degradationskernel (im Frequenzbereich)
$F$ ...‚Äã Urspr√ºngliches Bild (im Frequenzbereich)

---

**3.4 Muss bei der Richardson-Lucy-Deconvolution der Faltungskernel bekannt sein, der das St√∂rsignal verursacht? Erl√§utern Sie in eigenen Konzepten grob, wie mittels RLD ein verzerrtes Bild wieder rekonstruiert werden kann.**

Der Faltungskernel (PSF - Point Spread Function) $h$ muss bei der Richardson-Lucy-Deconvolution bekannt sein.

Die RLD rekonstruiert verzerrte Bilder durch einen iterativen Maximum-Likelihood-Algorithmus:

Grundprinzip:

- **Ausgangspunkt**: Das verzerrte Bild $g$ und der bekannte PSF-Kernel $h$
- **Ziel**: Rekonstruktion des urspr√ºnglichen Bildes $f$ aus der Beziehung $g = f * h$

1. Startwert: Erste Sch√§tzung $g^0$ (z.B. das beobachtete Bild oder ein Bild mit durchschnittlichem Grauwert (z.Bsp.: $127$))
2. Iteration:
    - Berechnung eines Zwischenbildes $c^t$ durch Faltung der aktuellen Sch√§tzung mit dem PSF: $c^t = h * g^t$
    - Pixelweise Berechnung von Korrekturkoeffizienten durch Vergleich mit dem beobachteten Bild: $m^{t(i,j)} = f(i,j)/c^{t(i,j)}$
    - Aktualisierung der Sch√§tzung: $g^{(t+1)} = g^t ¬∑ (m^t * h)$, wobei die Korrekturkoeffizienten mit dem PSF gefaltet werden
3. Abbruch: Der Algorithmus stoppt bei Konvergenz zu einer stabilen L√∂sung d.h. wenn sich die Korrekturkoeffizienten dem Wert 1.0 n√§hern

Daher ist das rechtzeitige Stoppen bei Konvergenz entscheidend f√ºr optimale Ergebnisse, da bei √úber-Iterierung Artefakte entstehen k√∂nnen und somit die Ergebnisqualit√§t leidet.

FunFact: Die RLD ist besonders effektiv bei Anwendungen wo die PSF gut charakterisiert werden kann, z.Bsp. Astronomie.

---

**3.5 Was bedeutet Image Superresolution? Nennen Sie unterschiedliche Strategien und erl√§utern Sie das Grundprinzip sehr vereinfacht mit eigenen Worten. Wo liegen die Grenzen der Image Superresolution?**

Image Superresolution bezeichnet Verfahren zur Erh√∂hung der Aufl√∂sung von Bildern - das hei√üt, aus einem niedrig aufgel√∂sten Bild wird ein h√∂her aufgel√∂stes Bild erzeugt, das mehr Details und sch√§rfere Strukturen enth√§lt.

Zwei Hauptkategorien:

- Klassische Computer Vision Ans√§tze:
    - Bi-kubische Interpolation: Einfache mathematische Vergr√∂√üerung
    - MAP (Maximum A-Posteriori): Kombination mehrerer leicht versetzter Bilder einer Szene
    - Bayes-Inferenz: Statistische Rekonstruktionsverfahren
    - SRCNN (Super-Resolution Convolutional Neural Network)
    - FSRCNN (Fast Super-Resolution CNN)
    - SRGAN (Super-Resolution Generative Adversarial Network)
    - RCAN (Residual Channel Attention Network)

Grundprinzip **sehr** vereinfacht erkl√§rt:

- Klassische Ans√§tze: Nutzen mathematische Interpolation oder kombinieren mehrere Bilder derselben Szene aus leicht unterschiedlichen Blickwinkeln, um fehlende Pixelinformationen zu "erraten".

- Deep Learning Ans√§tze: Neuronale Netze werden mit tausenden von Bildpaaren (niedrig-/hochaufgel√∂st) trainiert und lernen dabei, typische Muster und Strukturen zu erkennen. Bei neuen Bildern k√∂nnen sie dann "intelligente Vermutungen" √ºber fehlende Details anstellen.


Grenzen der Image Superresolution:

- Informationsgrenzen: Man kann keine Informationen erzeugen, die urspr√ºnglich nicht vorhanden waren - nur intelligente Sch√§tzungen basierend auf gelernten Mustern
- Anwendungsabh√§ngigkeit: Erfolg h√§ngt stark vom Bildinhalt ab (Text, Gesichter, unbekannte Objekte)
- Qualit√§tsverschlechterung: Bei zu vielen Iterationen oder zu hohen Vergr√∂√üerungsfaktoren k√∂nnen Artefakte entstehen
- Training-Bias: Deep Learning Modelle funktionieren am besten bei Bildinhalten, die ihren Trainingsdaten √§hneln
- Physikalische Grenzen: Das Nyquist-Shannon-Abtasttheorem setzt fundamentale Grenzen f√ºr die rekonstruierbare Information

> Das Nyquist-Shannon-Abtasttheorem besagt, dass man ein Signal nur dann vollst√§ndig rekonstruieren kann, wenn man es mit mindestens der doppelten Frequenz seiner h√∂chsten enthaltenen Frequenzkomponente abtastet - das hei√üt, wenn wichtige Details im urspr√ºnglichen Bild zu fein waren und nicht erfasst wurden, k√∂nnen sie auch nicht mehr rekonstruiert werden.

---
## 4. Localization [10]

**4.1 Er√∂rtern Sie die Kernidee von VSLAM anhand einer eigenen Skizze und f√ºhren Sie in diesem Zusammenhang √ºber die loop closure aus. In welchen Anwendungsdom√§nen spielt der VSLAM Algorithmus eine zentrale Rolle?**

Visual SLAM verwendet haupts√§chlich visuelle Informationen, also Kameradaten, zur simultanen Lokalisierung und Kartierung. Kameras erfassen visuelle Merkmale der Umgebung, und der Algorithmus berechnet daraus die Position des Ger√§ts.

- Ziel: Lokalisierung des Roboters und simultane Erstellung einer Karte der Umgebung.
- Input: Visuelle Sensordaten (monokular/stereo), oft kombiniert mit interner Odometrie.
- Herausforderung: Henne-Ei-Problem ‚Äî F√ºr die Lokalisierung wird eine Karte ben√∂tigt; f√ºr die Kartierung wird eine pr√§zise Lokalisierung vorausgesetzt.

**Ablauf und Konzept**
<p float="center">
    <img src="./img/vslam_1.png" width="100" />
    <img src="./img/vslam_2.png" width="100" />
</p>

1. Start: Roboter beginnt bei Pose (0,0); Umgebung ist unbekannt.
2. Bewegung gem√§√ü interner Odometrie (Positionssch√§tzung), erste Unsicherheiten entstehen.
3. Beobachtung erster Merkmale (z.B. durch SIFT); Position dieser Features ist nur ungenau bekannt.
4. Weitere Bewegung: Fehler in Pose-Sch√§tzung akkumulieren.
5. **Loop Closure:** Ein zuvor gesehenes Merkmal wird erneut erkannt. Dies erlaubt Korrektur von Pfad und Karte durch Optimierung.
    - F√ºhrt zu signifikanter Reduktion kumulierter Fehler.
    - Erh√∂ht Vertrauen in die Lokalisierung und die Kartengenauigkeit.
    - Ist entscheidend f√ºr globale Konsistenz der Karte.


**Anwendungsdom√§nen von VSLAM**
- Autonome Fahrzeuge ‚Äì Navigation ohne GPS, z.B. in Tunneln oder St√§dten.
- Rettungsroboter ‚Äì Kartierung unbekannter Gebiete (z.B. H√∂hlen, eingest√ºrzte Geb√§ude).
- Haushaltsroboter ‚Äì Staubsauger, Rasenm√§her: systematische Umgebungserfassung.
- Augmented Reality / Virtual Reality ‚Äì Echtzeitlokalisierung von mobilen Kameras.
- Drohnen / UAVs ‚Äì Umgebungserkennung f√ºr autonome Flugnavigation.

---

**4.2 Hough-Transformation von Linien (konventionelle Geradengleichung ‚Äì keine Polarkoordinaten): Linie im Ortsraum wird zu Punkt im Parameterraum. Geradengleichung + Skizze. Wie kann nun damit eine Linie als lokales Max. im Parameterraum detektiert werden (Skizze)**

Ein Punkt im Ortsraum $(x_0, y_0)$ entspricht einer Geraden im Parameterraum $(m, b)$, gem√§√ü:

  $$
  b = -x_0 \cdot m + y_0
  $$

* F√ºr jedes Pixel im Bild (Ortsraum) wird diese Gleichung genutzt, um alle m√∂glichen Geraden zu berechnen, die durch diesen Punkt verlaufen k√∂nnten -> **je ein Linienzug im Parameterraum**.

* **Im Parameterraum**:
  * Wenn sich viele dieser Linien in einem Punkt schneiden, bedeutet das, dass mehrere Pixel im Bild auf derselben Linie im Ortsraum liegen.
  * Der Schnittpunkt ist ein lokales Maximum im Akkumulator, das eine detektierte Linie repr√§sentiert.

<p float="center">
    <img src="./img/hough_line_detection.png" width="600" />
</p>

---

**4.3 Welche Parameter sind bei der Darstellung von Kreisen im Parameterraum sinnvoll.**

Ein Kreis im Ortsraum wird durch folgende Parameter beschrieben:
* $a, b$: Mittelpunkt des Kreises
* $r$: Radius


**Wie manifestiert sich ein Punkt dabei im Parameterraum?**

Punkt im Ortsraum -> Kreis im Parameterraum:

* Ein einzelner Bildpunkt $(x_0, y_0)$ k√∂nnte auf vielen m√∂glichen Kreisen liegen.
* F√ºr jeden m√∂glichen Radius $r$ ergibt sich eine Kreislinie im Parameterraum (a, b), auf der der Mittelpunkt (des Kreises aus dem Ortsraum) liegen m√ºsste.
* Das hei√üt: **Ein Punkt im Ortsraum wird zu einer Kreislinie im Parameterraum**.

**Wie sind dadurch Kreise zu detektieren? Ev. BSP**

* F√ºr viele Punkte im Bild (z.‚ÄØB. Kantenpunkte): Berechne alle m√∂glichen Kreismittelpunkte f√ºr feste Radien.
* Im **dreidimensionalen Parameterraum $(a, b, r)$**:

  * Viele Punkte erzeugen viele Kreislinien.
  * Wo sich viele dieser Linien schneiden, liegt ein lokales Maximum -> ein echter Kreis ist erkannt.

<p float="center">
    <img src="./img/hough_circle_detection.png" width="600" />
</p>

---

**4.4 Warum wird die Generalized Hough Transformation ben√∂tigt?**

Die normale Hough-Transformation funktioniert nur bei einfachen Formen (Linien, Kreise). F√ºr beliebige Formen (z.‚ÄØB. komplizierte Konturen) braucht man die generalizierte Hough Transformation.

**Wie wird die R-Tabelle errechnet? Hat die R-Tabelle die Bedeutung eines ‚ÄûForm-Modelles‚Äú?**

* Ein **Referenzpunkt** $P_c = (X_c, Y_c)$ wird gew√§hlt (z.‚ÄØB. Mittelpunkt).
* F√ºr jeden **Konturpunkt** $P_i = (X_i, Y_i)$:

  * Berechne den Abstand $r_i$ zum Referenzpunkt.
  * Bestimme die Kantenrichtung $\phi$ (z.‚ÄØB. mit Sobel-Operator).
  * Berechne den Winkel $\alpha_i$ zwischen $P_i$ und $P_c$.
* Die Werte $(r_i, \alpha_i)$ werden in der R-Tabelle nach Winkel $\phi$ sortiert gespeichert.

Ja, die R-Tabelle ist das Modell der Form.

<p float="center">
    <img src="./img/hough_general_1.png" width="200" />
    <img src="./img/hough_general_2.png" width="400" />
</p>


**Erh√∂ht sich die Genauigkeit des Modells, wenn mehr Punkte entlang der Kontur in die R-Tabelle verspeichert werden?**

Ja, mehr Punkte in der R-Tabelle = genaueres Modell.


**Was ist die Bedeutung vom Index phi und wie kann man die lokale Steigung der Kontur berechnen.**

œÜ ist der Winkel der Kante am Konturpunkt (lokale Richtung), berechnet aus dem **Gradienten** ($g_x, g_y$). Damit sucht man passende Vektoren in der R-Tabelle.


**Wie berechnet man die lokale Steigung der Kontur?**
Mit den **x- und y-Gradienten**, z.‚ÄØB. aus dem **Sobel-Filter**:

$$
\phi = \arctan\left(\frac{g_y}{g_x}\right)
$$

**Erlaubt die R-Tabelle das Rekonstruieren von Mittelpunkten ($P_c$) der Form?**
Ja, durch R√ºckrechnung mit den gespeicherten Werten $(r_i, \alpha_i)$ und dem Winkel $\phi$.

---

**4.5 Was sind gute lokale Features in Bildern und warum?**

Gute lokale Features sind Ecken oder Kanten, also Bildbereiche mit starken √Ñnderungen in mehreren Richtungen. Homogene Fl√§chen sind ungeeignet, da sie keine markanten Merkmale enthalten.

<p float="center">
    <img src="./img/image_features.png" width="400" />
</p>

**Motivieren Sie die Bedeutung von lokalen Features f√ºr unterschiedliche Anwendungsdom√§nen (Panoramafotos, 3D Rekonstruktion, Tracking, Objekterkennung,‚Ä¶).**

Sie helfen, markante Punkte wiederzuerkennen, z.‚ÄØB. f√ºr:
- Panoramafotos: Bilder zusammensetzen
- 3D-Rekonstruktion: gleiche Punkte aus verschiedenen Blickwinkeln (mehrere Kameras)
- Tracking: Objektverfolgung √ºber Zeit
- Objekterkennung: bekannte Formen wiederfinden
- Robotik (z.‚ÄØB. visuelle Navigation)


**Welche Anforderungen und Qualit√§tsindikatoren werden an lokale Features gestellt?**

- Repetitivit√§t: gleiche Punkte zuverl√§ssig finden, auch in mehreren Bildern
- Affine Invarianz: robust gegen Translation, Rotation, Skalierung
- Robustheit: stabil trotz Rauschen, Unsch√§rfe, Helligkeits√§nderung
- Unverwechselbarkeit (Distinctiveness): sollte einzigartige Strukturen zeigen
- Ausgewogene Anzahl: genug, um das Bild gut zu beschreiben, aber nicht zu viele
- Effizienz: schnell genug f√ºr Echtzeitanwendungen (z.‚ÄØB. Tracking)

---

**4.6 Welche Features werden beim Hessian/Harris Corner Detector erkannt und wie?** 
Beide Verfahren erkennen Ecken, also Punkte mit starkem Intensit√§tswechsel in mehreren Richtungen.

- Hessian-Detektor: Schaut sich an, wie stark sich Helligkeit in x- und y-Richtung √§ndert (mit 2. Ableitungen). Wenn sich in beiden Richtungen viel ver√§ndert, erkennt er eine Ecke.
- Harris-Detektor: Nutzt 1. Ableitungen (also wie schnell Helligkeit sich √§ndert) und baut daraus eine Matrix. Wenn sich die Helligkeit in mehreren Richtungen gleichzeitig stark √§ndert, sind beide Eigenwerte gro√ü, das ist eine Ecke. Die Ecke wird erkannt, wenn ein gewisser Schwellwert √ºberschritten wird.


**Welche Bedeutung haben in diesem Zusammenhang Gradienten, die Hesse Matrix sowie Eigenwerte?**
- Gradienten: Zeigen, wie stark und in welcher Richtung sich die Helligkeit im Bild √§ndert (1. Ableitungen in x- und y-Richtung). Grundlage f√ºr die Eckenerkennung, da Ecken starke Helligkeitswechsel in mehreren Richtungen zeigen.

- Hesse-Matrix (Hessian): Verwendet die 2. Ableitungen der Helligkeit (also wie stark sich der Gradient selbst √§ndert). Gibt Auskunft dar√ºber, ob die √Ñnderung stark genug ist und in welchen Richtungen Struktur im Bild vorhanden ist. Eine gro√üe Determinante der Hesse-Matrix bedeutet, dass sich die Helligkeit in x- und y-Richtung stark √§ndert -> m√∂gliche Ecke.

- Eigenwerte (bei Harris & Hessian): Zeigen die St√§rke der √Ñnderungen in den Hauptachsen (Richtungen) der Matrix.
    - Beide Eigenwerte gro√ü: starke √Ñnderung in zwei Richtungen -> Ecke
    - Ein Eigenwert gro√ü, der andere klein: Kante
    - Beide klein: Fl√§che ohne Struktur


**Welche Anwendungsdom√§nen gibt es, bei denen Feature-Detektion essentiell ist?**

- Panoramafotos: Bilder zusammensetzen
- 3D-Rekonstruktion: gleiche Punkte aus verschiedenen Blickwinkeln (mehrere Kameras)
- Tracking: Objektverfolgung √ºber Zeit
- Objekterkennung: bekannte Formen wiederfinden
- Robotik (z.‚ÄØB. visuelle Navigation)


---

**4.7 Was bedeutet scale invariance?**

Scale Invariance (Skalierungsunabh√§ngigkeit) bedeutet, dass ein Feature-Detektor Merkmale erkennt, egal wie gro√ü oder klein das Objekt im Bild erscheint (z.B. durch Entfernung oder Zoom).

Klassische Detektoren wie Harris/Hessian sind nicht scale-invariant. Ein Objekt, das bei kleiner Skalierung ein scharfes Detail zeigt, kann bei gr√∂√üerer Skalierung glatt und unauff√§llig wirken -> wird dann nicht mehr als Feature erkannt.


**Wie kann man Skalierungsunabh√§ngigkeit erzielen, vgl. Filterpyramide, SIFT etc.**

L√∂sung: Verwendung von Scale Spaces
-> Bilder werden in verschiedenen Aufl√∂sungen (Skalen) betrachtet.

<p float="center">
    <img src="./img/scale_invariance_scale_spaces.png" width="400" />
</p>

1. Filterpyramide: 
- Das Bild wird mehrfach gegl√§ttet (durch Gaussian-Filter mit verschiedenen œÉ) und runterskaliert.
- Feature-Detektion wird auf allen Ebenen wiederholt. So kann man Features in verschiedenen Gr√∂√üen finden.

2. SIFT (Scale-Invariant Feature Transform): Baut einen skalierbaren Bildraum auf, indem das Bild mit verschiedenen Gaussian-Filtern bearbeitet wird. Aus den gefilterten Bildern werden dann Differenzbilder (Difference of Gaussian, DoG) gebildet.


**Erl√§utern Sie Difference of Gaussian und wie dadurch Skalierungsunabh√§ngigkeit erzielt werden kann.**

DoG ist die **Differenz zweier gegl√§tteter Bilder**, die mit zwei leicht unterschiedlichen Sigma-Werten gefiltert wurden. Man zieht das st√§rker gegl√§ttete Bild vom weniger gegl√§tteten ab.

<p float="center">
    <img src="./img/dog_1.png" width="400" />
</p>

**Warum macht man das?**: Diese Differenz hebt Bereiche hervor, in denen sich die Helligkeit im Bild stark √§ndert, das sind Kanten oder Ecken.

**Skalierungsunabh√§ngigkeit**:
* Indem man DoG auf verschiedenen Skalen (also f√ºr viele verschiedene œÉ) berechnet, betrachtet man das Bild aus mehreren "Entfernungen".
* Auf kleinen Skalen sieht man feine Details, auf gro√üen Skalen sieht man grobe Strukturen.
* Sucht man nun lokale Maxima oder Minima im Raum aus (x, y, œÉ), findet man Punkte, die auf allen Skalen gut erkennbar sind. Diese Punkte sind skalierungsinvariant, weil sie nicht von der Gr√∂√üe des Objekts abh√§ngen, sondern von seiner Struktur √ºber Skalen hinweg.

In SIFT nutzt man genau diese DoG-Extrema als stabile Merkmale, die man auch dann wiederfindet, wenn ein Objekt im Bild gr√∂√üer oder kleiner erscheint. So ist der Algorithmus robust gegen√ºber Zoom oder Entfernung.

---

**4.8 Wie wird bei SIFT der 128-Element Feature-Vektor errechnet.**

1. **Vorbereitung (Steps 1-3):**
SIFT detektiert zun√§chst stabile Keypoints √ºber mehrere Skalen (DoG-Pyramide), lokalisiert sie pr√§zise und weist jedem Keypoint eine dominante Orientierung zu, um Rotationsinvarianz zu gew√§hrleisten.

2. **Step 4 ‚Äì Keypoint Description:**
* Um jeden Keypoint wird ein 16x16 Pixel gro√ües lokales Umfeld betrachtet.
* Dieses Umfeld wird in 16 kleine Regionen (4x4 Pixel) aufgeteilt.
* In jeder Region wird ein Histogramm der lokalen Gradientenorientierungen mit 8 Bins erstellt.
* Die Gradientenwerte werden mit einer Gau√ü-Gewichtung und der Gradientenst√§rke gewichtet.
* Die 16 Histogramme mit je 8 Werten werden aneinandergereiht -> ergibt den 128-dimensionalen Feature-Vektor (16 x 8 = 128).
* Abschlie√üend wird der Vektor normalisiert, um Beleuchtungs√§nderungen robust zu begegnen.
    <p float="center">
        <img src="./img/sift_step_4.png" width="300" />
    </p>


**Was bedeutet in diesem Zusammenhang Feature-Trajektorie und wie werden Features diesbez√ºglich ‚Äûgematched‚Äú?**

Feature-Trajektorie: Die Verbindung eines SIFT-Features aus Bild A mit dem korrespondierenden Feature aus Bild B, also die Zuordnung von Merkmalen zwischen zwei Bildern.

Matching:
* Beim Matching sucht man f√ºr jeden Feature-Vektor aus Bild A den √§hnlichsten Feature-Vektor aus Bild B (meist mit der euklidischen Distanz).
* Um sicherzugehen, dass die Zuordnung zuverl√§ssig ist, wird der Ratio-Test angewendet: Nur wenn der beste Match deutlich besser ist als der zweitbeste (z.B. mindestens 20% besser), wird die Verbindung akzeptiert.
* So entstehen stabile Feature-Trajektorien zwischen Bildern.



**Angenommen es werden zwei Mengen an SIFT-Features mittels Feature-Trajektorie in Verbindung gesetzt ‚Äì wie kann die Ergebnisqualit√§t zus√§tzlich verbessert werden?**

Clusterbildung und Hough-Transform:
* Features, die zusammen √§hnliche Bewegungen oder Transformationsparameter (z.B. affine Transformation) aufweisen, werden zu Clustern zusammengefasst.
* Mittels Hough-Transform stimmen die Keypoints √ºber m√∂gliche Objektpositionen und -orientierungen ab.
* Der Cluster mit den meisten "Votes" repr√§sentiert die wahrscheinlichste √úbereinstimmung, was die Zuverl√§ssigkeit gegen√ºber Ausrei√üern und falschen Matches deutlich erh√∂ht.

---

**4.9 Erl√§utern Sie das Histogram of Oriented Gradients (HOG).**

HOG ist ein Verfahren zur Beschreibung von Bildern, das vor allem f√ºr die Erkennung von Objekten (z.B. Personen) eingesetzt wird. Es analysiert die Verteilung von Kantenrichtungen in kleinen Bereichen eines Bildes.

**Welche zentralen Features werden hierbei zur Berechnung des L2-normalisierten 36-bin Feature-Vektors herangezogen?**

1. **Gradientenberechnung:**

* Zuerst werden horizontale und vertikale Gradienten mit Sobel-Filtern berechnet.
* Daraus ergeben sich f√ºr jeden Pixel die Gradientenst√§rke (Magnitude) und die Gradientenrichtung.
    <p float="center">
        <img src="./img/hog_1.png" width="200" />
    </p>


2. **Histogramme pro Pixelzelle:**

* Das Bild wird in kleine Zellen von z.B. 8√ó8 Pixeln unterteilt.
* F√ºr jede Zelle wird ein Histogramm der Gradientenrichtungen berechnet, aufgeteilt in 9 Bins (also 9 Winkelbereiche von 0¬∞ bis 180¬∞, da die Gradientenrichtung "unsigned" ist).
    <img src="./img/hog_2.png" width="600" />
* Die Gradienten werden gewichtet nach ihrer St√§rke und auf die beiden n√§chsten Bins verteilt (Interpolation).
    <img src="./img/hog_3.png" width="600" />
    

3. **Block-Normalisierung:**

* Vier benachbarte Zellen (also 2x2 Zellen = 16√ó16 Pixel) werden zusammengefasst zu einem Block.
* Die 4 Histogramme (je 9 Bins) ergeben einen Vektor mit 36 Werten (4√ó9).
* Dieser Vektor wird L2-normalisiert (auf die L√§nge 1 skaliert), um Beleuchtungsunterschiede auszugleichen (beschreibt nur die relative Verteilung der Kantenrichtungen, nicht die absolute St√§rke).
    <img src="./img/hog_4.png" width="600" />

4. **Endergebnis:**

* Das Bild wird von vielen solchen Bl√∂cken abgedeckt, und die 36-Element-Vektoren dieser Bl√∂cke werden aneinandergereiht, um den endg√ºltigen HOG-Feature-Vektor zu bilden.


**Wie wird HOG invariant bzgl. affiner Transformationen wie etwa Skalierung, Translation oder Rotation?**

* **Skalierung:**
    Durch Vorverarbeitung wird das Bild auf eine standardisierte Gr√∂√üe skaliert, sodass die Gr√∂√üe der Zellen und Bl√∂cke relativ zum Bild konstant bleibt.

* **Translation (Verschiebung):**
    HOG arbeitet lokal in kleinen Zellen und Bl√∂cken, deshalb verschieben sich die lokalen Histogramme mit dem Bildinhalt mit und bleiben konsistent.

* **Rotation:**
    HOG verwendet ‚Äûunsigned‚Äú Gradienten (0¬∞ bis 180¬∞), wodurch eine 180¬∞-Rotation oder Spiegelung weniger Einfluss auf den Histogrammverlauf hat.


---

**4.10 Erl√§utern Sie die Bedeutung von Bild-Charakteristika zur Segmentierung/Klassifikation. Warum muss dabei i.d.R. ein Feature-Vektor verwendet werden, um mehrere Klassen in robuster Weise voneinander trennen zu k√∂nnen.**

Man nutzt mehrere Bild-Charakteristika gleichzeitig in einem Feature-Vektor, um verschiedene Klassen zuverl√§ssig voneinander zu trennen, da einzelne Merkmale oft zu ungenau oder irref√ºhrend sind. Mehr Merkmale = bessere Unterscheidungskraft.

* **Bild-Charakteristika** sind Merkmale oder Eigenschaften eines Bildes (z.B. Farbe, Textur, Kanten, Form), die helfen, verschiedene Bildbereiche oder Objekte zu unterscheiden.
* Bei der **Segmentierung** werden Bildbereiche in sinnvolle Teile (z.B. Himmel, Boden, Objekte) getrennt.
* Bei der **Klassifikation** wird jedem Bild oder Bildteil eine Kategorie (Klasse) zugewiesen, z.B. ‚ÄûAuto‚Äú, ‚ÄûMensch‚Äú oder ‚ÄûHintergrund‚Äú.

Warum ein Feature-Vektor:
* Ein einzelnes Merkmal (z.B. nur Farbe) ist oft nicht ausreichend, weil verschiedene Klassen sich in einem Merkmal √ºberlappen k√∂nnen.
* Ein Feature-Vektor fasst viele verschiedene Merkmale zusammen (z.B. Farbe, Textur, Form, Kantenmuster). Dadurch kann man besser Unterschiede erkennen.

---
## 5. 3D Rekonstruktion [6]

**5.1 Nennen Sie Verfahren, um mit einzelnen monokularen Bildern bzw. mehrerer monokularer Bilder Objekte 3D zu rekonstruieren. Vergleichen Sie die Ans√§tze.**

**Einzelne monokulare Bilder:**
Man hat 2D-Bild und Kamera Kalibrierung aber keine Tiefeninformation

* **Gr√∂√üenbestimmung via Referenzobjekte - Size from orthogonal marker**
    * Ma√üstabssch√§tzung durch bekannte Objekte (z.B. Lineal, Checkerboard)
    * Wichtig f√ºr Kamera-Kalibrierung, forensische Anwendungen

* **Shape from shading (SfS)**
    * Physikalisches Modell hinsichtlich Lichtrichtung, Ansichtsrichtung und Oberfl√§cheneigenschaften 
    * Sch√§tzung der Oberfl√§chentiefe aus Helligkeitsverl√§ufen
    * Nutzt verschiedene Reflektionsmodelle (z.B. Phong) -> kann reverse-engineered werden mit CGI
    * Ungef√§hre Ann√§herung der Gradienten / Topographie, aber keine absoluten Distanzen 

* **[Deep] Depth from Focus (DFF, DDFF)**
    * Idee: Objekte, die weiter weg sind, erscheinen unscharf auf dem Bild -> Tiefe aus der Bildsch√§rfe
    * Funktioniert mit Fokus-Stapel -> mehrere Bilder mit verschiedenen Sch√§rfeebenen
    * Schwierig erreichbar bei modernen Ger√§ten (auto-focus mus deaktiviert werden)
    * Ergebnisse sind beeinflusst von Textur und Schatten

* **Shape from Texture: Structured Light**
    * Idee: Wenn eine regelm√§√üige Textur perspektivisch verzerrt ist, kann daraus Tiefe gesch√§tzt werden.
    * Structured Light: Projektor + 2 Kameras + codiertes Lichtmuster
    * Probleme bei Okklusionen, daher hierarchische Rekonstruktion n√∂tig
    * Alternative: Tiem of Flight (ToF)

* **Depth-Sensor + Structure from Motion am Beispiel von Microsoft Kinect**
    * Kinect verwendet IR-Tiefenkamera, um die Entfernung von Oberfl√§chen zu messen
    * Kombination von Tiefenbild + Farbbild ergibt Textur und Tiefenbild

* **Deep Learning**
    * Netzwerke sch√§tzen relative Tiefe aus Bildinhalten
    * Nutzung von CNNs (z.B. U-Net), trainiert auf CGI-Daten
    * 2D‚Äì3D-Rekonstruktion √ºber morphbare Modelle (z.B. Gesicht, Haare)

**Mehrere monokulare Bilder:**
Hier wird echte 3D-Tiefe rekonstruiert, da mehrere Perspektiven genutzt werden

* **Epipolargeometrie**
    * Ein Punkt im ersten Bild liegt auf einer Epipolarlinie im zweiten.
    * Schnittpunkte mehrerer Linien -> rekonstruierbare 3D-Punkte
    * Basis f√ºr viele 3D-Verfahren

    <img src="img/3d_epipolar.png" width="400" />

* **Stereo Matching**
    * Zwei nahe aneinanderliegende Kameras mit bekannter Distanz sind auf die selbe Szene gerichtet -> basierend auf dem Prinzip der menschlichen binokularen Tiefenwahrnehmung 
    * Disparit√§tskarte durch horizontale Verschiebung der Bildpunkte
    * Tiefe √ºber Parallaxeneffekt (Verschiebung der Position eines Objekts, wenn es aus verschiedenen Blickwinkeln betrachtet wird)
    * Probleme bei niedriger Textur oder einfarbigen Fl√§chen

    <img src="img/3d_stereo_matching.png" width="400" />

* **Bekannte Kameraposition - Silhouette Reconstruction**
    * Nutzung mehrerer Segmentierungsbilder (Silhouetten) zur Rekonstruktion der Visual Hull
    * Erfordert keine Punktkorrespondenz - nur bin√§re Segmentierungsmasken
    * Beispiel: K√∂rpermodell aus 8 Kameraansichten (alle 45¬∞), Person rotiert
    * Ergebnis: grob konvexe Form, konkave Teile nicht erfasst

    <img src="img/3d_silhouette.png" width="400" />

* **Structure from Motion (SfM)**
    * Aus vielen Bildern (z.‚ÄØB. Video von Drohne) wird:
        * Kamera-Position gesch√§tzt
        * 3D-Struktur rekonstruiert
    * Funktioniert mit unbekannter Kameraposition
    * Beispiel: Gel√§nde-Topografie aus Flugvideo
    * Bessere Ergebnisse als Silhouette Reconstruction m√∂glich, wegen nicht senkrechter Sicht  
    * Skalierung bleibt oft unklar, kann aber mit GPS verbessert werden

> **Fazit:** Mit einzelnen monokularen Bildern kann man Tiefe nur approximieren, mit mehreren Bildern kann man echte 3D-Tiefe rekonstruieren.

---

**5.2 Nennen Sie Anwendungsgebiete der 3D Rekonstruktion. Geben Sie eine Definition f√ºr "3D Rekonstruktion" in eigenen Worten. Erl√§utern Sie, warum in 2D Bildern die Gr√∂√üe von Objekten nicht ableitbar ist**

**3D Rekonstruktion:** bezeichnet das Verfahren, bei dem aus Objekten oder Szenen aus 2D-Bildern, Videos oder anderen Sensordaten ein 3D-Modelle erstellt wird

**Anwendungsgebiete:**
- Medizin & Industrie
    - Diagnostik durch CT/MRT
    - 3D Modelle von Organen
- 3D printing, 3D modelling
- Bewegungserfassung
- AR/VR
- Robotik & Navigation
    - Pfadplanung 
    - Lokalisierung 
- ...

**Warum kann man aus einem 2D-Bild die Objektgr√∂√üe nicht direkt ableiten?**
- In 2D-Bildern werden 3D-Szenen perspektivisch auf 2D-Fl√§che projiziert
- Dabei geht Tiefeninformation (in Z-Richtung) verloren
- Ohne zus√§tzliche Informationen (z.‚ÄØB. Kameraparameter, Referenzobjekte, ...) ist die reale Gr√∂√üe nicht berechenbar

---

**5.3 F√ºhren Sie die Gr√∂√üenbestimmung in Bildern via Referenzobjekte aus. Erl√§utern Sie Shape from Shading.**
(Wiederholung von Teilen aus 5.1)

* **Gr√∂√üenbestimmung via Referenzobjekte - Size from orthogonal marker**
    * Ma√üstabssch√§tzung durch bekannte Objekte (z.B. Lineal, Checkerboard)
    * Wichtig f√ºr Kamera-Kalibrierung, forensische Anwendungen

* **Shape from shading (SfS)**
    * Physikalisches Modell hinsichtlich Lichtrichtung, Ansichtsrichtung und Oberfl√§cheneigenschaften
    * Sch√§tzung der Oberfl√§chentiefe aus Helligkeitsverl√§ufen
    * Nutzt verschiedene Reflektionsmodelle (z.B. Phong) -> kann reverse-engineered werden mit CGI
    * Ungef√§hre Ann√§herung der Gradienten / Topographie, aber keine absoluten Distanzen 

---

**5.4 Erl√§utern Sie Deep Depth from Focus. Erl√§utern Sie Shape from texture / structured light**
(Wiederholung von Teilen aus 5.1)

* **[Deep] Depth from Focus (DFF, DDFF)**
    * Idee: Objekte, die weiter weg sind, erscheinen unscharf auf dem Bild -> Tiefe aus der Bildsch√§rfe
    * Funktioniert mit Fokus-Stapel -> mehrere Bilder mit verschiedenen Sch√§rfeebenen
    * Schwierig erreichbar bei modernen Ger√§ten (auto-focus mus deaktiviert werden)
    * Ergebnisse sind beeinflusst von Textur und Schatten

* **Shape from Texture: Structured Light**
    * Idee: Wenn eine regelm√§√üige Textur perspektivisch verzerrt ist, kann daraus Tiefe gesch√§tzt werden.
    * Structured Light: Projektor + 2 Kameras + codiertes Lichtmuster
    * Probleme bei Okklusionen, daher hierarchische Rekonstruktion n√∂tig
    * Alternative: Tiem of Flight (ToF)

---

**5.5 F√ºhren Sie √ºber den Einsatz von Deep Learning zur Bestimmung der Tiefe / 3D Form aus.**
(Wiederholung von Teilen aus 5.1)

* **Deep Learning**
    * Netzwerke sch√§tzen relative Tiefe aus Bildinhalten
    * Nutzung von CNNs (z.B. U-Net), trainiert auf CGI-Daten
    * 2D‚Äì3D-Rekonstruktion √ºber morphbare Modelle (z.B. Gesicht, Haare)

---

**5.6 Erl√§utern Sie Stereo Matching und Structure from Motion. Welche Bilder sind daf√ºr gut geeignet? Erl√§utern Sie die Silhouette Reconstruction.**
(teilweise Wiederholung von Teilen aus 5.1)

* **Stereo Matching**
    * Zwei nahe aneinanderliegende Kameras mit bekannter Distanz sind auf die selbe Szene gerichtet -> basierend auf dem Prinzip der menschlichen binokularen Tiefenwahrnehmung 
    * Disparit√§tskarte durch horizontale Verschiebung der Bildpunkte
    * Tiefe √ºber Parallaxeneffekt (Verschiebung der Position eines Objekts, wenn es aus verschiedenen Blickwinkeln betrachtet wird)
    * Ungeeignete Bilder: 
        * Texturarme/einfarbige Bereiche
    * Geeignete Bilder: 
        * Strukturreiche Szenen mit ausreichender Textur
        * Gleichm√§√üige Beleuchtung
        * Statische Szenen ohne Bewegung

* **Structure from Motion (SfM)**
    * Aus vielen Bildern (z.‚ÄØB. Video von Drohne) wird:
        * Kamera-Position gesch√§tzt
        * 3D-Struktur rekonstruiert
    * Funktioniert mit unbekannter Kameraposition
    * Beispiel: Gel√§nde-Topografie aus Flugvideo
    * Bessere Ergebnisse als Silhouette Reconstruction m√∂glich, wegen nicht senkrechter Sicht  
    * Skalierung bleibt oft unklar, kann aber mit GPS verbessert werden
    * Geeignete Bilder: 
        * Verschiedenene Blickwinkeln
        * Nicht-orthogonale Ansichten f√ºr bessere Rekonstruktionsqualit√§t
        * √úberlappende Bildsequenzen
        * Strukturreiche Szenen
        * Gleichm√§√üige Beleuchtung

* **Bekannte Kameraposition - Silhouette Reconstruction**
    * Nutzung mehrerer Segmentierungsbilder (Silhouetten) zur Rekonstruktion der Visual Hull
    * Erfordert keine Punktkorrespondenz - nur bin√§re Segmentierungsmasken
    * Beispiel: K√∂rpermodell aus 8 Kameraansichten (alle 45¬∞), Person rotiert
    * Ergebnis: grob konvexe Form, konkave Teile nicht erfasst

---
## 6. Computer Vision and Machine Learning [9]

**6.1 Wof√ºr werden Bilddatenbanken als ‚Äûground-truth‚Äú ben√∂tigt.**

Ground-truth bedeutet die exakte Referenz, also eine genaue, verl√§ssliche "Wahrheit" √ºber Bildinhalte (z.B. korrekte Segmentierung, Klassifikation).

Bilddatenbanken mit Ground-truth werden gebraucht, um:
  * Algorithmen in der Computer Vision zu trainieren und zu testen.
  * Ergebnisse verschiedener Verfahren objektiv zu vergleichen.
  * Zu √ºberpr√ºfen, wie gut eine Methode wirklich arbeitet.

**Welche Daten sind dabei hilfreich (vgl. Bilddaten, Depth-from-Focus, 3D Rekonstruktion).**

* Bilddaten mit genauen Labels (z.B. was ist Objekt, Hintergrund, welche Klasse).
* Depth-from-Focus Daten: zus√§tzliche Tiefeninformationen, um Objekte besser zu unterscheiden.
* 3D Rekonstruktion: 3D-Modelle oder Volumen helfen, r√§umliche Strukturen exakt zu erfassen.

---

**6.2 Was sind die Fallstricke bei der Verwendung von Bilddatenbanken?**

* **Begrenzte Realit√§tsn√§he:** Bilddatenbanken zeigen nur einen kleinen Ausschnitt der realen Welt, viele Situationen fehlen.
* **Qualit√§t der Daten:** Ground-truth ist nicht immer korrekt oder vollst√§ndig, es gibt Fehler, L√ºcken und Verzerrungen.
* **Bias:** Bilder k√∂nnen z.B. bevorzugte Blickwinkel (Autos nur von der Seite) oder typische Posen enthalten, was das Training einseitig macht.
* **Redundanz:** Viele Daten enthalten sehr √§hnliche oder gleiche Motive, was die Vielfalt einschr√§nkt und Modelle √ºberoptimiert auf diese speziellen Beispiele.
* **Visuelle Herausforderungen:** Probleme wie Schatten, Blendungen, Reflexionen, schlechte Beleuchtung oder Verdeckungen werden oft nicht oder nur unzureichend abgebildet.
* **Unterschiedliche Test- und Realit√§tsszenarien:** Algorithmen, die in Benchmarks gut abschneiden, k√∂nnen im echten Einsatz stark schlechter sein.

---

**6.3 Wie kann man Datenbanken ‚Äûanreichern‚Äú, wenn nicht gen√ºgend Testdaten verf√ºgbar sind?**

1. Datenaugmentation durch Transformationen:

* Bilder spiegeln (horizontal/vertikal)
* Helligkeit, Kontrast oder Farben ver√§ndern (z.B. Gamma-Korrektur)
* Affine Transformationen: drehen, verschieben, skalieren, verzerren
* K√ºnstliches Hinzuf√ºgen von Rauschen oder Objekten
* Sub-Bilder (Ausschnitte) von wichtigen Bildteilen erstellen
* Ersetzen von Bildteilen, z.B. Gesichter in Videos austauschen
* Einf√ºgen von computergenerierten Bildern (CGI) in bestehende Szenen

2. Automatisches Hinzuf√ºgen √§hnlicher Bilder aus dem Internet:

* Bilder von Suchmaschinen (Google, Flickr) abrufen
* Automatische Annotation (Labeln) mit wenig manuellem Aufwand durch √Ñhnlichkeitssuche
* Schrittweise Erweiterung des Datensatzes basierend auf √Ñhnlichkeit (Feedback-Loop)

3. Transfer Learning mit vortrainierten Deep CNNs:

* Vortrainierte Netzwerke (z.B. auf ImageNet) als Ausgangspunkt nehmen
* Auf kleinere, dom√§nenspezifische Datens√§tze (z.B. Blumenarten) weitertrainieren (Feinabstimmung)
* Vorteil: Gute Grundkenntnisse werden erweitert, es braucht weniger neue Daten

4. Nutzung von 3D-Modellen zur Generierung von Bildern:

* 3D-Objekte digital rendern (realistische Bilder erzeugen)
* Verschiedene Perspektiven und Posen simulieren (z.B. f√ºr Menschen, Fahrzeuge)
* Dadurch kann der Datensatz gezielt und vielf√§ltig erweitert werden

**Was ist diesbez√ºglich der Vorteil/Nachteil von simulierten [Bild]daten?**

Vorteile von simulierten Bilddaten (z.B. aus 3D-Modellen)

* Kontrollierte Vielfalt: Man kann gezielt verschiedene Ansichten, Posen, Beleuchtungen erzeugen.
* Unbegrenzte Datenmenge: Es k√∂nnen beliebig viele Bilder generiert werden.
* Exakte Annotation: Da alles simuliert ist, sind Labels und Tiefeninformationen perfekt verf√ºgbar.
* Geringerer Aufwand f√ºr manuelles Labeln.

Nachteile von simulierten Bilddaten

* Realit√§tsl√ºcke: Simulierte Bilder k√∂nnen von echten Bildern abweichen (Dom√§nenunterschiede).
* Weniger nat√ºrliche Variationen: Manche Bildfehler oder St√∂rungen fehlen (z.B. Kamerarauschen, Reflexionen).
* Aufwendige Erstellung: Hochrealistische 3D-Modelle und Rendering ben√∂tigen viel Aufwand und Rechenleistung.

---

**6.4 F√ºhren Sie detailliert √ºber Datenaugmentierung sowie Tools zur semi-automatischen Annotation von Bildern aus.**

Datenaugmentierung ist eine Methode, um vorhandene Bilddaten k√ºnstlich zu erweitern, indem Variationen der Originalbilder erzeugt werden. Ziel ist es, die Datenmenge zu erh√∂hen und gleichzeitig die Robustheit von Modellen gegen unterschiedliche Bildvariationen zu verbessern.

Methoden: Spiegeln, Drehen, Skalieren, Farb- und Kontrast√§nderungen, Rauschen hinzuf√ºgen, Zufallsausschnitte.

<img src="./img/data_augmentation.png" width="600" />

Semi-automatische Annotation:
Der Computer schl√§gt automatische Annotationen vor, die der Mensch dann korrigiert oder best√§tigt.

Tools: Polygon- oder Bounding-Box-Annotation (LabelMe, LabelImg), Superpixel-Segmentierung, Deep-Learning-Modelle f√ºr Vorschl√§ge (Mask R-CNN, CVAT).

---

**6.5 Was ist ein U-NET?**

Ein U-Net ist ein Convolutional Neural Network (CNN) speziell f√ºr pr√§zise Bildsegmentierung (z.B. in Medizin).

- Es basiert auf einem Encoder-Decoder-Prinzip:
    - Downsampling-Pfad (Encoder): extrahiert Merkmale durch Faltung (Convolution).
    - Upsampling-Pfad (Decoder): rekonstruiert Bildaufl√∂sung zur pr√§zisen Segmentierung.
- √úber Skip-Connections werden Feature Maps derselben Aufl√∂sung direkt verbunden -> erm√∂glicht pr√§zise Lokalisierung bei gleichzeitigem semantischem Verst√§ndnis.
- Das Modell gibt meist eine Wahrscheinlichkeitskarte aus -> bin√§re Schwelle f√ºr endg√ºltiges Segmentierungsergebnis notwendig.

<p float="center">
    <img src="./img/u-net.png" width="300" />
    <img src="./img/u-net_2.png" width="250" />
</p>
<img src="./img/u-net_3.png" width="550" />

**Wie w√ºrden Sie dabei den Datenfluss mit RNNs bzw. LSTM vergleichen?**

- U-Net: Intra-sample Verbindungen -> verarbeitet r√§umliche Information innerhalb eines Bildes.
- RNNs/LSTMs: Inter-sample Verbindungen -> verarbeiten zeitliche Abh√§ngigkeiten √ºber mehrere Datenpunkte hinweg (z.‚ÄØB. bei Videos oder Sequenzen).
- W√§hrend U-Nets "horizontal" im Bild lernen, lernen LSTMs "vertikal" √ºber Zeit oder Sequenzen hinweg.


**Welche Vorverarbeitungsschritte sind notwendig, bevor das Eingangssignal (z.B. Bild) dem Input-Layer √ºbergeben werden kann?**

1. BLOB-Conversion:
- Alle Bilder m√ºssen gleiche Gr√∂√üe, Farbtiefe und Wertebereich (z.‚ÄØB. 8-bit, 0‚Äì255) haben.
- Die Wahl der Interpolationsmethode beim Resizing (z.‚ÄØB. bilinear vs. nearest) hat Einfluss auf Ergebnisqualit√§t.

2. Mean-Subtraction:
- Durchschnittswert des Bildes oder der Trainingsmenge wird abgezogen.
- Ziel: Farb- und Beleuchtungsunterschiede ausgleichen.
- Optional: Standardabweichungsskalierung zur Normalisierung √ºber Kan√§le hinweg.

3. Image Whitening (Zero Component Analysis):
- Zentriert die Daten (Mittelwert = 0),
- macht sie unkorreliert (durch Rotation),
- skaliert sie (durch Eigenwert-Normalisierung).
- Ziel: Reduktion von Redundanz und Korrelation in den Inputdaten, was das Lernen stabiler macht.

**Welche Auswirkung hat die Wahl der Vorverarbeitung auf das trainierte CNN?**
- Essentiell f√ºr die Trainingsqualit√§t:
    - Falsche Vorverarbeitung -> schlechte Ergebnisse trotz guter Architektur.
- Gute Vorverarbeitung f√ºhrt zu:
    - robusteren Modellen,
    - besserer Generalisation,
    - schnellerer und stabilerer Konvergenz.
- Spezielle Vorverarbeitung (z.‚ÄØB. Augmentation) kann Dom√§nenwissen (z.‚ÄØB. Schnee im Wald) einflie√üen lassen.

---

**6.6 Nennen Sie Anwendungsgebiete f√ºr Deep Learning.**

* Allgemeine Computer Vision-Aufgaben:
    * Pose- und Aktivit√§tserkennung
    * Bildklassifikation, Segmentierung und Objekterkennung
* Spezifische Erkennungsaufgaben:
    * Gesichtserkennung und Emotionsklassifikation
    * Formbasierte Detektion/Klassifikation (z.B. f√ºr abstrakte Formen)
    * Personen-Detektion & -Tracking
    * Erkennung von Objekten im Kontext des autonomen Fahrens (z.B. Autos, Fahrzeuge)
    * Texterkennung (OCR)
    * Spracherkennung und nat√ºrliche Sprachverarbeitung
    * Human 3D Pose Estimation, Roboterwahrnehmung und Viewpoint Estimation unter Verwendung von 3D-Modellen
    * Klassifikation von Meta-Merkmalen wie der Anzahl von "Inseln" in Bildern
* Synthese und Generierung von Daten:
    * Bildsynthese mithilfe von GANs (Generative Adversarial Networks), z.B. f√ºr die Erzeugung von Trainingsbildern oder die Synthese von Gesichtern oder medizinischen Datens√§tzen.
    * Text-to-Speech-Synthese.

**Differenzieren Sie dabei zwischen Klassifikation und Segmentierung ‚Äì was sind die Auswirkungen auf die Netzwerkarchitektur (input/output Layer)?**

* **Klassifikation:**
    * **Ziel:** Einer Eingabe (z.B. einem Bild) ein oder mehrere Labels (Klassen) zuordnen.
    * **Auswirkungen auf die Netzwerkarchitektur (Output Layer):** Bei der Klassifikation erzeugt die Ausgabeschicht des Convolutional Neural Networks (CNN) typischerweise nur Klassenlabels. Das bedeutet, das Netzwerk gibt eine Wahrscheinlichkeitsverteilung √ºber vordefinierte Klassen aus (z.B. "Katze", "Hund", "Vogel").

* **Segmentierung:**
    * **Ziel:** Jedem Pixel eines Bildes eine Klasse zuweisen, um so Objekte oder Regionen exakt abzugrenzen.
    * **Auswirkungen auf die Netzwerkarchitektur (Input/Output Layer):** F√ºr die Segmentierung wird die Bilddaten nach der Gr√∂√üenreduzierung wieder auf die urspr√ºngliche Aufl√∂sung hochskaliert, um eine finale Segmentierungsmaske zu erhalten.

**Was ist die Schwierigkeit bei der Verarbeitung von Eingangsdaten dynamischer Gr√∂√üe?**

Die Verarbeitung von Eingangsdaten dynamischer Gr√∂√üe (d.h., unterschiedlich gro√üe Bilder) stellt f√ºr viele Deep Learning-Modelle, insbesondere f√ºr Convolutional Neural Networks (CNNs), eine Herausforderung dar:

* **Feste Eingabegr√∂√üen:** CNNs sind typischerweise f√ºr eine spezifische Eingabebildgr√∂√üe trainiert. Das bedeutet, dass alle Eingabebilder an diese vordefinierte Gr√∂√üe angepasst werden m√ºssen, bevor sie in das Netzwerk eingespeist werden.
* **Preprocessing (BLOB-Konvertierung):** Der erste Schritt im Preprocessing f√ºr CNNs ist die BLOB-Konvertierung, bei der die Gr√∂√üe, die Pixeltiefe und der Skalarbereich aller Eingabebilder angepasst werden.
    * Die Wahl der Interpolationsstrategie w√§hrend dieser Gr√∂√üenanpassung kann einen erheblichen Einfluss auf die erzielbare Ausgabequalit√§t haben.

---

**6.7 Erl√§utern Sie das Prinzip von GANs und deren Einsatzgebiete.**

Generative Adversarial Networks (GANs) sind eine Art von neuronalen Netzen, die sich hervorragend zum Generieren von Daten eignen, wie zum Beispiel k√ºnstliche Musik oder k√ºnstliche Gem√§lde, die echten Werken √§hneln. Das grundlegende Prinzip von GANs basiert auf einem **adversariellen (gegnerischen) Ansatz**, bei dem zwei neuronale Netzwerke in einem kontinuierlichen Wettstreit miteinander trainiert werden.

Diese beiden Netzwerke sind:
* Der **Generator**: Dieses Netzwerk hat die Aufgabe, eine sehr gro√üe Anzahl von Hypothesen (z.B. neue Bilder) zu generieren.
* Der **Diskriminator**: Dieses Netzwerk entscheidet, ob der vom Generator erzeugte Inhalt einem gegebenen Trainingsdatensatz √§hnelt, d.h., ob er "echt" oder "gef√§lscht" ist.

<img src="./img/gan.png" width="300" />

**Der Trainingsprozess:**

Der Trainingsprozess ist ein dynamisches Gleichgewicht, bei dem sich beide Netzwerke **kontinuierlich verbessern**:
* Der **Generator** wird immer besser darin, √ºberzeugendere Daten zu erzeugen, die den "Reality Check" des Diskriminators bestehen k√∂nnen.
* Der **Diskriminator** wird gleichzeitig immer besser darin, "schlechten" (k√ºnstlichen) Inhalt zu erkennen.

Dieser Prozess kann auch mithilfe eines **Multi-Resolution-Ansatzes** trainiert werden, insbesondere da die GAN-Gewichte sehr empfindlich auf √Ñnderungen reagieren. Das Training erfolgt dabei in ansteigender Aufl√∂sung, beginnend beispielsweise von 4x4 Pixeln bis zu 128x128 Pixeln und h√∂her.

<img src="./img/gan_mulitres.png" width="300" />


**Einsatzgebiete von GANs:**

GANs finden in einer Vielzahl von Anwendungsgebieten Einsatz, insbesondere dort, wo die Synthese und Generierung von Daten im Vordergrund steht:
* **Bildsynthese**:
    * GANs k√∂nnen zur **Erzeugung von Trainingsbildern** verwendet werden, was den Bedarf an gro√üen Mengen manuell annotierter Daten reduziert.
    * Ein prominentes Beispiel ist die **Gesichtssynthese**. Hierbei k√∂nnen GANs auf der Basis von beispielsweise 200.000 unausgerichteten RGB-Bildern von Prominenten trainiert werden, um realistische Gesichter zu generieren.
    * Die **Synthese medizinischer Datens√§tze** ist ein weiteres wichtiges Anwendungsgebiet. GANs erm√∂glichen das Training mit CT-Schnitten zusammen mit zugeh√∂rigen Ground-Truth-Segmentierungen. Dies stellt eine ausgezeichnete Alternative dar, wenn nicht gen√ºgend Trainingsdaten vorhanden sind.
* **Text-to-Speech-Synthese**

---

**6.8 Diskutieren Sie die Bedeutung und Anwendungsgebiete von Yolo.**

**YOLO** (You Only Look Once) ist eine Architektur f√ºr die **Objektdetektion in Echtzeit**. Es wurde entwickelt, um Objekte in Bildern nicht nur zu klassifizieren, sondern auch deren genaue Position und Gr√∂√üe mittels **Bounding Boxes** zu lokalisieren.

<img src="./img/yolo_1.png" width="200" />

Die Kernidee von YOLO ist, dass ein Eingabebild in ein **Raster unterteilt** wird. Jede Zelle in diesem Raster ist daf√ºr verantwortlich, ein Objekt zu detektieren. Dabei werden f√ºr jedes Rasterfeld **Begrenzungsrahmen (Bounding Boxes)** und **Objektklassen-Wahrscheinlichkeiten** erkannt. Die Begrenzungsrahmen werden dann basierend auf ihrer Konfidenz gefiltert.

<img src="./img/yolo_2.png" width="400" />

Anwendungsf√§lle:
- Echtzeit Objektdetektion
- z.B. erkennen von Baumst√§mmen in einem Bild

---

**6.9 Diskutieren Sie die Bedeutung der Vorverarbeitung, wenn Bilddaten in den Eingangstensor √ºberf√ºhrt werden.**

* Vorverarbeitung ist Grundlage f√ºr plausible, robuste und pr√§zise Ergebnisse.
* Anpassung an das CNN-Layout: Die gew√§hlte Vorverarbeitungsstrategie muss stark vom Layout des CNNs und der Art und Weise, wie die Regionen von Interesse (ROIs) der Eingabebilder in das Netzwerk eingespeist werden, abh√§ngen.
* **Standardisierung des Inputs (BLOB-Konvertierung):**
    * Anpassung von Gr√∂√üe, Pixeltiefe und Skalarbereich aller Eingabebilder.
    * Die Wahl der Interpolationsstrategie kann die erreichbare Ausgabequalit√§t erheblich beeinflussen.
* **Farb-/Helligkeitsnormalisierung (Mittelwert-Subtraktion):**
    * Durch das Subtrahieren des Mittelwerts des Bildes wird eine normierte Skala erreicht.
    * Dies gleicht Helligkeits√§nderungen in den Datens√§tzen aus.
    * Zus√§tzlich kann eine Skalierung durch die Standardabweichung angewendet werden, um gleiche Wertebereiche pro Kanal zu gew√§hrleisten.
* **Merkmalsverst√§rkung (Image Whitening):**
    * Diese Methode, auch Zero Component Analysis genannt, transformiert die Daten so, dass die Kovarianzmatrix der Einheitsmatrix entspricht.
    * Dazu werden die Daten null-zentriert, entkorreliert (rotiert, bis keine Korrelation mehr besteht) und neu skaliert.


**Was k√∂nnte dabei schiefgehen?**

* **Inad√§quate Strategiewahl:** Wenn eine ungeeignete Vorverarbeitungsstrategie gew√§hlt wird, k√∂nnen schwache Ergebnisse die Folge sein.
* **Falsche Interpolation:** Eine schlechte Wahl der Interpolationsstrategie w√§hrend der BLOB-Konvertierung kann die Ausgabequalit√§t signifikant beeintr√§chtigen.
* **Mangelnde Repr√§sentativit√§t des Datensatzes:** Auch umfassende Datens√§tze decken oft nur einen sehr kleinen Teil der realen Welt ab.
* **Qualit√§t und Korrektheit der Referenzdaten:** Die Qualit√§t und Korrektheit der zugrundeliegenden Referenzdatens√§tze (Ground Truth) wird selten selbst evaluiert. Dies kann zu Redundanzen, L√ºcken und Fehlern im Ground Truth f√ºhren, die einen Bias einf√ºhren.
* **Herausfordernde Bildmerkmale:** Die Algorithmen k√∂nnen bei Bildern mit geringem Kontrast, Schatten, Blendung, Reflexionen, verwirrenden Texturen und Okklusionen Schwierigkeiten haben.
* **Bias in Orientierungen oder Inhalten:** Ein Bias hinsichtlich bevorzugter Orientierungen (z.B. Autos immer von der Seite) oder Redundanzen im Inhalt.

---